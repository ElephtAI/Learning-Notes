<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><style>body {
  max-width: 980px;
  border: 1px solid #ddd;
  outline: 1300px solid #fff;
  margin: 16px auto;
}

body .markdown-body
{
  padding: 45px;
}

@font-face {
  font-family: fontawesome-mini;
  src: url(data:font/woff;charset=utf-8;base64,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) format('woff');
}

.markdown-body {
  font-family: sans-serif;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
  color: #333333;
  overflow: hidden;
  font-family: "Helvetica Neue", Helvetica, "Segoe UI", Arial, freesans, sans-serif;
  font-size: 16px;
  line-height: 1.6;
  word-wrap: break-word;
}

.markdown-body a {
  background: transparent;
}

.markdown-body a:active,
.markdown-body a:hover {
  outline: 0;
}

.markdown-body b,
.markdown-body strong {
  font-weight: bold;
}

.markdown-body mark {
  background: #ff0;
  color: #000;
  font-style: italic;
  font-weight: bold;
}

.markdown-body sub,
.markdown-body sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
.markdown-body sup {
  top: -0.5em;
}
.markdown-body sub {
  bottom: -0.25em;
}

.markdown-body h1 {
  font-size: 2em;
  margin: 0.67em 0;
}

.markdown-body img {
  border: 0;
}

.markdown-body hr {
  -moz-box-sizing: content-box;
  box-sizing: content-box;
  height: 0;
}

.markdown-body pre {
  overflow: auto;
}

.markdown-body code,
.markdown-body kbd,
.markdown-body pre,
.markdown-body samp {
  font-family: monospace, monospace;
  font-size: 1em;
}

.markdown-body input {
  color: inherit;
  font: inherit;
  margin: 0;
}

.markdown-body html input[disabled] {
  cursor: default;
}

.markdown-body input {
  line-height: normal;
}

.markdown-body input[type="checkbox"] {
  box-sizing: border-box;
  padding: 0;
}

.markdown-body table {
  border-collapse: collapse;
  border-spacing: 0;
}

.markdown-body td,
.markdown-body th {
  padding: 0;
}

.markdown-body .codehilitetable {
  border: 0;
  border-spacing: 0;
}

.markdown-body .codehilitetable tr {
  border: 0;
}

.markdown-body .codehilitetable pre,
.markdown-body .codehilitetable div.codehilite {
  margin: 0;
}

.markdown-body .linenos,
.markdown-body .code,
.markdown-body .codehilitetable td {
  border: 0;
  padding: 0;
}

.markdown-body td:not(.linenos) .linenodiv {
  padding: 0 !important;
}

.markdown-body .code {
  width: 100%;
}

.markdown-body .linenos div pre,
.markdown-body .linenodiv pre,
.markdown-body .linenodiv {
  border: 0;
  -webkit-border-radius: 0;
  -moz-border-radius: 0;
  border-radius: 0;
  -webkit-border-top-left-radius: 3px;
  -webkit-border-bottom-left-radius: 3px;
  -moz-border-radius-topleft: 3px;
  -moz-border-radius-bottomleft: 3px;
  border-top-left-radius: 3px;
  border-bottom-left-radius: 3px;
}

.markdown-body .code div pre,
.markdown-body .code div {
  border: 0;
  -webkit-border-radius: 0;
  -moz-border-radius: 0;
  border-radius: 0;
  -webkit-border-top-right-radius: 3px;
  -webkit-border-bottom-right-radius: 3px;
  -moz-border-radius-topright: 3px;
  -moz-border-radius-bottomright: 3px;
  border-top-right-radius: 3px;
  border-bottom-right-radius: 3px;
}

.markdown-body * {
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}

.markdown-body input {
  font: 13px Helvetica, arial, freesans, clean, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol";
  line-height: 1.4;
}

.markdown-body a {
  color: #4183c4;
  text-decoration: none;
}

.markdown-body a:hover,
.markdown-body a:focus,
.markdown-body a:active {
  text-decoration: underline;
}

.markdown-body hr {
  height: 0;
  margin: 15px 0;
  overflow: hidden;
  background: transparent;
  border: 0;
  border-bottom: 1px solid #ddd;
}

.markdown-body hr:before,
.markdown-body hr:after {
  display: table;
  content: " ";
}

.markdown-body hr:after {
  clear: both;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  margin-top: 15px;
  margin-bottom: 15px;
  line-height: 1.1;
}

.markdown-body h1 {
  font-size: 30px;
}

.markdown-body h2 {
  font-size: 21px;
}

.markdown-body h3 {
  font-size: 16px;
}

.markdown-body h4 {
  font-size: 14px;
}

.markdown-body h5 {
  font-size: 12px;
}

.markdown-body h6 {
  font-size: 11px;
}

.markdown-body blockquote {
  margin: 0;
}

.markdown-body ul,
.markdown-body ol {
  padding: 0;
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body ol ol,
.markdown-body ul ol {
  list-style-type: lower-roman;
}

.markdown-body ul ul ol,
.markdown-body ul ol ol,
.markdown-body ol ul ol,
.markdown-body ol ol ol {
  list-style-type: lower-alpha;
}

.markdown-body dd {
  margin-left: 0;
}

.markdown-body code,
.markdown-body pre,
.markdown-body samp {
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  font-size: 12px;
}

.markdown-body pre {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body kbd {
  background-color: #e7e7e7;
  background-image: -moz-linear-gradient(#fefefe, #e7e7e7);
  background-image: -webkit-linear-gradient(#fefefe, #e7e7e7);
  background-image: linear-gradient(#fefefe, #e7e7e7);
  background-repeat: repeat-x;
  border-radius: 2px;
  border: 1px solid #cfcfcf;
  color: #000;
  padding: 3px 5px;
  line-height: 10px;
  font: 11px Consolas, "Liberation Mono", Menlo, Courier, monospace;
  display: inline-block;
}

.markdown-body>*:first-child {
  margin-top: 0 !important;
}

.markdown-body>*:last-child {
  margin-bottom: 0 !important;
}

.markdown-body .headerlink {
  font: normal 400 16px fontawesome-mini;
  vertical-align: middle;
  margin-left: -16px;
  float: left;
  display: inline-block;
  text-decoration: none;
  opacity: 0;
  color: #333;
}

.markdown-body .headerlink:focus {
  outline: none;
}

.markdown-body h1 .headerlink {
  margin-top: 0.8rem;
}

.markdown-body h2 .headerlink,
.markdown-body h3 .headerlink {
  margin-top: 0.6rem;
}

.markdown-body h4 .headerlink {
  margin-top: 0.2rem;
}

.markdown-body h5 .headerlink,
.markdown-body h6 .headerlink {
  margin-top: 0;
}

.markdown-body .headerlink:hover,
.markdown-body h1:hover .headerlink,
.markdown-body h2:hover .headerlink,
.markdown-body h3:hover .headerlink,
.markdown-body h4:hover .headerlink,
.markdown-body h5:hover .headerlink,
.markdown-body h6:hover .headerlink {
  opacity: 1;
  text-decoration: none;
}

.markdown-body h1 {
  padding-bottom: 0.3em;
  font-size: 2.25em;
  line-height: 1.2;
  border-bottom: 1px solid #eee;
}

.markdown-body h2 {
  padding-bottom: 0.3em;
  font-size: 1.75em;
  line-height: 1.225;
  border-bottom: 1px solid #eee;
}

.markdown-body h3 {
  font-size: 1.5em;
  line-height: 1.43;
}

.markdown-body h4 {
  font-size: 1.25em;
}

.markdown-body h5 {
  font-size: 1em;
}

.markdown-body h6 {
  font-size: 1em;
  color: #777;
}

.markdown-body p,
.markdown-body blockquote,
.markdown-body ul,
.markdown-body ol,
.markdown-body dl,
.markdown-body table,
.markdown-body pre,
.markdown-body .admonition {
  margin-top: 0;
  margin-bottom: 16px;
}

.markdown-body hr {
  height: 4px;
  padding: 0;
  margin: 16px 0;
  background-color: #e7e7e7;
  border: 0 none;
}

.markdown-body ul,
.markdown-body ol {
  padding-left: 2em;
}

.markdown-body ul ul,
.markdown-body ul ol,
.markdown-body ol ol,
.markdown-body ol ul {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body li>p {
  margin-top: 16px;
}

.markdown-body dl {
  padding: 0;
}

.markdown-body dl dt {
  padding: 0;
  margin-top: 16px;
  font-size: 1em;
  font-style: italic;
  font-weight: bold;
}

.markdown-body dl dd {
  padding: 0 16px;
  margin-bottom: 16px;
}

.markdown-body blockquote {
  padding: 0 15px;
  color: #777;
  border-left: 4px solid #ddd;
}

.markdown-body blockquote>:first-child {
  margin-top: 0;
}

.markdown-body blockquote>:last-child {
  margin-bottom: 0;
}

.markdown-body table {
  display: block;
  width: 100%;
  overflow: auto;
  word-break: normal;
  word-break: keep-all;
}

.markdown-body table th {
  font-weight: bold;
}

.markdown-body table th,
.markdown-body table td {
  padding: 6px 13px;
  border: 1px solid #ddd;
}

.markdown-body table tr {
  background-color: #fff;
  border-top: 1px solid #ccc;
}

.markdown-body table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

.markdown-body img {
  max-width: 100%;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}

.markdown-body code,
.markdown-body samp {
  padding: 0;
  padding-top: 0.2em;
  padding-bottom: 0.2em;
  margin: 0;
  font-size: 85%;
  background-color: rgba(0,0,0,0.04);
  border-radius: 3px;
}

.markdown-body code:before,
.markdown-body code:after {
  letter-spacing: -0.2em;
  content: "\00a0";
}

.markdown-body pre>code {
  padding: 0;
  margin: 0;
  font-size: 100%;
  word-break: normal;
  white-space: pre;
  background: transparent;
  border: 0;
}

.markdown-body .codehilite {
  margin-bottom: 16px;
}

.markdown-body .codehilite pre,
.markdown-body pre {
  padding: 16px;
  overflow: auto;
  font-size: 85%;
  line-height: 1.45;
  background-color: #f7f7f7;
  border-radius: 3px;
}

.markdown-body .codehilite pre {
  margin-bottom: 0;
  word-break: normal;
}

.markdown-body pre {
  word-wrap: normal;
}

.markdown-body pre code {
  display: inline;
  max-width: initial;
  padding: 0;
  margin: 0;
  overflow: initial;
  line-height: inherit;
  word-wrap: normal;
  background-color: transparent;
  border: 0;
}

.markdown-body pre code:before,
.markdown-body pre code:after {
  content: normal;
}

/* Admonition */
.markdown-body .admonition {
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  position: relative;
  border-radius: 3px;
  border: 1px solid #e0e0e0;
  border-left: 6px solid #333;
  padding: 10px 10px 10px 30px;
}

.markdown-body .admonition table {
  color: #333;
}

.markdown-body .admonition p {
  padding: 0;
}

.markdown-body .admonition-title {
  font-weight: bold;
  margin: 0;
}

.markdown-body .admonition>.admonition-title {
  color: #333;
}

.markdown-body .attention>.admonition-title {
  color: #a6d796;
}

.markdown-body .caution>.admonition-title {
  color: #d7a796;
}

.markdown-body .hint>.admonition-title {
  color: #96c6d7;
}

.markdown-body .danger>.admonition-title {
  color: #c25f77;
}

.markdown-body .question>.admonition-title {
  color: #96a6d7;
}

.markdown-body .note>.admonition-title {
  color: #d7c896;
}

.markdown-body .admonition:before,
.markdown-body .attention:before,
.markdown-body .caution:before,
.markdown-body .hint:before,
.markdown-body .danger:before,
.markdown-body .question:before,
.markdown-body .note:before {
  font: normal normal 16px fontawesome-mini;
  -moz-osx-font-smoothing: grayscale;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  line-height: 1.5;
  color: #333;
  position: absolute;
  left: 0;
  top: 0;
  padding-top: 10px;
  padding-left: 10px;
}

.markdown-body .admonition:before {
  content: "\f056\00a0";
  color: 333;
}

.markdown-body .attention:before {
  content: "\f058\00a0";
  color: #a6d796;
}

.markdown-body .caution:before {
  content: "\f06a\00a0";
  color: #d7a796;
}

.markdown-body .hint:before {
  content: "\f05a\00a0";
  color: #96c6d7;
}

.markdown-body .danger:before {
  content: "\f057\00a0";
  color: #c25f77;
}

.markdown-body .question:before {
  content: "\f059\00a0";
  color: #96a6d7;
}

.markdown-body .note:before {
  content: "\f040\00a0";
  color: #d7c896;
}

.markdown-body .admonition::after {
  content: normal;
}

.markdown-body .attention {
  border-left: 6px solid #a6d796;
}

.markdown-body .caution {
  border-left: 6px solid #d7a796;
}

.markdown-body .hint {
  border-left: 6px solid #96c6d7;
}

.markdown-body .danger {
  border-left: 6px solid #c25f77;
}

.markdown-body .question {
  border-left: 6px solid #96a6d7;
}

.markdown-body .note {
  border-left: 6px solid #d7c896;
}

.markdown-body .admonition>*:first-child {
  margin-top: 0 !important;
}

.markdown-body .admonition>*:last-child {
  margin-bottom: 0 !important;
}

/* progress bar*/
.markdown-body .progress {
  display: block;
  width: 300px;
  margin: 10px 0;
  height: 24px;
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  border-radius: 3px;
  background-color: #ededed;
  position: relative;
  box-shadow: inset -1px 1px 3px rgba(0, 0, 0, .1);
}

.markdown-body .progress-label {
  position: absolute;
  text-align: center;
  font-weight: bold;
  width: 100%; margin: 0;
  line-height: 24px;
  color: #333;
  text-shadow: 1px 1px 0 #fefefe, -1px -1px 0 #fefefe, -1px 1px 0 #fefefe, 1px -1px 0 #fefefe, 0 1px 0 #fefefe, 0 -1px 0 #fefefe, 1px 0 0 #fefefe, -1px 0 0 #fefefe, 1px 1px 2px #000;
  -webkit-font-smoothing: antialiased !important;
  white-space: nowrap;
  overflow: hidden;
}

.markdown-body .progress-bar {
  height: 24px;
  float: left;
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  border-radius: 3px;
  background-color: #96c6d7;
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, .5), inset 0 -1px 0 rgba(0, 0, 0, .1);
  background-size: 30px 30px;
  background-image: -webkit-linear-gradient(
    135deg, rgba(255, 255, 255, .4) 27%,
    transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%,
    transparent 77%, transparent
  );
  background-image: -moz-linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
  background-image: -ms-linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
  background-image: -o-linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
  background-image: linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
}

.markdown-body .progress-100plus .progress-bar {
  background-color: #a6d796;
}

.markdown-body .progress-80plus .progress-bar {
  background-color: #c6d796;
}

.markdown-body .progress-60plus .progress-bar {
  background-color: #d7c896;
}

.markdown-body .progress-40plus .progress-bar {
  background-color: #d7a796;
}

.markdown-body .progress-20plus .progress-bar {
  background-color: #d796a6;
}

.markdown-body .progress-0plus .progress-bar {
  background-color: #c25f77;
}

.markdown-body .candystripe-animate .progress-bar{
  -webkit-animation: animate-stripes 3s linear infinite;
  -moz-animation: animate-stripes 3s linear infinite;
  animation: animate-stripes 3s linear infinite;
}

@-webkit-keyframes animate-stripes {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 60px 0;
  }
}

@-moz-keyframes animate-stripes {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 60px 0;
  }
}

@keyframes animate-stripes {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 60px 0;
  }
}

.markdown-body .gloss .progress-bar {
  box-shadow:
    inset 0 4px 12px rgba(255, 255, 255, .7),
    inset 0 -12px 0 rgba(0, 0, 0, .05);
}

/* MultiMarkdown Critic Blocks */
.markdown-body .critic_mark {
  background: #ff0;
}

.markdown-body .critic_delete {
  color: #c82829;
  text-decoration: line-through;
}

.markdown-body .critic_insert {
  color: #718c00 ;
  text-decoration: underline;
}

.markdown-body .critic_comment {
  color: #8e908c;
  font-style: italic;
}

.markdown-body .headeranchor {
  font: normal normal 16px fontawesome-mini;
  line-height: 1;
  display: inline-block;
  text-decoration: none;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.headeranchor:before {
  content: '\e157';
}

.markdown-body .task-list-item {
  list-style-type: none;
}

.markdown-body .task-list-item+.task-list-item {
  margin-top: 3px;
}

.markdown-body .task-list-item input {
  margin: 0 4px 0.25em -20px;
  vertical-align: middle;
}

/* Media */
@media only screen and (min-width: 480px) {
  .markdown-body {
    font-size:14px;
  }
}

@media only screen and (min-width: 768px) {
  .markdown-body {
    font-size:16px;
  }
}

@media print {
  .markdown-body * {
    background: transparent !important;
    color: black !important;
    filter:none !important;
    -ms-filter: none !important;
  }

  .markdown-body {
    font-size:12pt;
    max-width:100%;
    outline:none;
    border: 0;
  }

  .markdown-body a,
  .markdown-body a:visited {
    text-decoration: underline;
  }

  .markdown-body .headeranchor-link {
    display: none;
  }

  .markdown-body a[href]:after {
    content: " (" attr(href) ")";
  }

  .markdown-body abbr[title]:after {
    content: " (" attr(title) ")";
  }

  .markdown-body .ir a:after,
  .markdown-body a[href^="javascript:"]:after,
  .markdown-body a[href^="#"]:after {
    content: "";
  }

  .markdown-body pre {
    white-space: pre;
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  .markdown-body pre,
  .markdown-body blockquote {
    border: 1px solid #999;
    padding-right: 1em;
    page-break-inside: avoid;
  }

  .markdown-body .progress,
  .markdown-body .progress-bar {
    -moz-box-shadow: none;
    -webkit-box-shadow: none;
    box-shadow: none;
  }

  .markdown-body .progress {
    border: 1px solid #ddd;
  }

  .markdown-body .progress-bar {
    height: 22px;
    border-right: 1px solid #ddd;
  }

  .markdown-body tr,
  .markdown-body img {
    page-break-inside: avoid;
  }

  .markdown-body img {
    max-width: 100% !important;
  }

  .markdown-body p,
  .markdown-body h2,
  .markdown-body h3 {
    orphans: 3;
    widows: 3;
  }

  .markdown-body h2,
  .markdown-body h3 {
    page-break-after: avoid;
  }
}
</style><script type='text/javascript' src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js'></script><script>MathJax.Hub.Config({
  config: ["MMLorHTML.js"],
  extensions: ["tex2jax.js"],
  jax: ["input/TeX", "output/HTML-CSS", "output/NativeMML"],
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true
  },
  TeX: {
    extensions: ["AMSmath.js", "AMSsymbols.js"],
    TagSide: "right",
    TagIndent: ".8em",
    MultLineWidth: "85%",
    equationNumbers: {
      autoNumber: "AMS",
    },
    unicode: {
      fonts: "STIXGeneral,'Arial Unicode MS'"
    }
  },
  displayAlign: "center",
  showProcessingMessages: false,
  messageStyle: 'none'
});</script><style>/*GitHub*/
.codehilite {background-color:#fff;color:#333333;}
.codehilite .hll {background-color:#ffffcc;}
.codehilite .c{color:#999988;font-style:italic}
.codehilite .err{color:#a61717;background-color:#e3d2d2}
.codehilite .k{font-weight:bold}
.codehilite .o{font-weight:bold}
.codehilite .cm{color:#999988;font-style:italic}
.codehilite .cp{color:#999999;font-weight:bold}
.codehilite .c1{color:#999988;font-style:italic}
.codehilite .cs{color:#999999;font-weight:bold;font-style:italic}
.codehilite .gd{color:#000000;background-color:#ffdddd}
.codehilite .ge{font-style:italic}
.codehilite .gr{color:#aa0000}
.codehilite .gh{color:#999999}
.codehilite .gi{color:#000000;background-color:#ddffdd}
.codehilite .go{color:#888888}
.codehilite .gp{color:#555555}
.codehilite .gs{font-weight:bold}
.codehilite .gu{color:#800080;font-weight:bold}
.codehilite .gt{color:#aa0000}
.codehilite .kc{font-weight:bold}
.codehilite .kd{font-weight:bold}
.codehilite .kn{font-weight:bold}
.codehilite .kp{font-weight:bold}
.codehilite .kr{font-weight:bold}
.codehilite .kt{color:#445588;font-weight:bold}
.codehilite .m{color:#009999}
.codehilite .s{color:#dd1144}
.codehilite .n{color:#333333}
.codehilite .na{color:teal}
.codehilite .nb{color:#0086b3}
.codehilite .nc{color:#445588;font-weight:bold}
.codehilite .no{color:teal}
.codehilite .ni{color:purple}
.codehilite .ne{color:#990000;font-weight:bold}
.codehilite .nf{color:#990000;font-weight:bold}
.codehilite .nn{color:#555555}
.codehilite .nt{color:navy}
.codehilite .nv{color:teal}
.codehilite .ow{font-weight:bold}
.codehilite .w{color:#bbbbbb}
.codehilite .mf{color:#009999}
.codehilite .mh{color:#009999}
.codehilite .mi{color:#009999}
.codehilite .mo{color:#009999}
.codehilite .sb{color:#dd1144}
.codehilite .sc{color:#dd1144}
.codehilite .sd{color:#dd1144}
.codehilite .s2{color:#dd1144}
.codehilite .se{color:#dd1144}
.codehilite .sh{color:#dd1144}
.codehilite .si{color:#dd1144}
.codehilite .sx{color:#dd1144}
.codehilite .sr{color:#009926}
.codehilite .s1{color:#dd1144}
.codehilite .ss{color:#990073}
.codehilite .bp{color:#999999}
.codehilite .vc{color:teal}
.codehilite .vg{color:teal}
.codehilite .vi{color:teal}
.codehilite .il{color:#009999}
.codehilite .gc{color:#999;background-color:#EAF2F5}
</style><title>Paper_sum</title></head><body><article class="markdown-body"><h1 id="summary-for-the-graph-network-relevant-readings">Summary for the Graph &amp; Network relevant readings<a class="headerlink" href="#summary-for-the-graph-network-relevant-readings" title="Permanent link"></a></h1>
<p>(1) <code>Jul 2018</code> <a href="https://www.ijcai.org/proceedings/2018/466">Automatic Opioid User Detection From Twitter: Transductive Ensemble Built On Different Meta-graph Based Similarities Over Heterogeneous Information Network</a> <em>[Yujie Fan, Yiming Zhang, Yanfang Ye, Xin Li]</em></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p>Opioid (e.g., heroin and morphine) addiction has become one of the largest and deadliest epidemics in the United States. To combat such deadly epidemic, in this paper, we propose a novel framework named <strong>HinOPU</strong> to automatically detect opioid users from Twitter, which will assist in sharpening our understanding toward the behavioral process of opioid addiction and treatment. In HinOPU, to model the users and the posted tweets as well as their rich relationships, we introduce <strong>structured heterogeneous information network</strong> (HIN) for representation. Afterwards, we use <strong>meta-graph</strong> based approach to characterize the semantic relatedness over users; we then formulate different similarities over users based on different meta-graphs on HIN. To reduce the cost of acquiring labeled samples for supervised learning, we propose a transductive classification method to build the base classifiers based on different similarities formulated by different meta-graphs. Then, to further improve the detection accuracy, we construct an <strong>ensemble</strong> to combine different predictions from different base classifiers for opioid user detection. Comprehensive experiments on real sample collections from Twitter are conducted to validate the effectiveness of HinOPU in opioid user detection by comparisons with other alternate methods.</p>
</blockquote>
<ul>
<li><strong>Key notes</strong>: <ul>
<li><strong>Structured heterogenoeous information network</strong> (<em>representation</em>)</li>
<li><strong>meta-graph</strong> (<em>characterize semantic relatedness over users, formulate similarities</em>)</li>
<li><strong>Transductive classification</strong> (<em>semi-supervised, reduce the cost of acquiring labeled samples, vs inductive models</em>)</li>
<li><strong>Ensemble method</strong></li>
</ul>
</li>
</ul>
<hr />
<p>(2) <code>Oct 2017</code> <a href="https://arxiv.org/abs/1710.10903">Graph Attention Network</a> <em>[Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio]</em></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p>We present <strong>graph attention networks (GATs)</strong>, novel neural network architectures that operate on graph-structured data, leveraging masked <strong>self-attentional layers</strong> to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which <u>nodes are able to attend over their neighborhoods&rsquo; features</u>, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of <strong>spectral-based graph neural networks</strong> simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).</p>
</blockquote>
<ul>
<li><strong>Key Notes</strong>:<ul>
<li>Graph Neural Network consists of an <strong>iterative process</strong>: <ul>
<li>propagates the node states until equilibrium</li>
<li>followed by a neural network, produces an output for each node on its states</li>
</ul>
</li>
<li><strong>Convolution</strong> to the graph domain: <ul>
<li><u>spectral approaches</u>: Computing the <strong>eigendecomposition of the graph Laplacian</strong> in Fourier domain, Chebyshev expansion; <strong>depends on spacial graph structure</strong></li>
<li><u>non-spectral approches</u>: define convolutions directly on the graph, operating on groups of spatially close neighbors -&gt; <a href="https://arxiv.org/abs/1706.02216">GraphSAGE</a></li>
</ul>
</li>
<li><strong>Attention-based</strong> architecture -&gt; <strong>node classification</strong>: <em>compute the hidden representations of each node in the graph by attending over its <strong>neighbors</strong> following a <strong>self-attention</strong> strategy</em></li>
<li>Interesting <strong>Properties</strong> of Attention-architecture:<ul>
<li><strong>[i]</strong> efficient operation, parallelizable across <strong>node-neighbor pairs</strong></li>
<li><strong>[ii]</strong> can be applied to graph nodes having different degrees (specifyig arbitrary weights to neighbors)</li>
<li><strong>[iii]</strong> directly applicable to inductive learning problems, the model has to generalize to completely unseen graphs</li>
</ul>
</li>
<li>In the attention layer, features from K <strong>independent attention mechanisms</strong> are concatenated to employ <strong>multi-head attention</strong> -&gt; <u>to stabilize the learning process of self-attention</u></li>
<li>The author&rsquo;s attention-layer, works with the <strong>entirety of the neighborhood</strong> and <strong>does not assume any ordering</strong> within it, and it&rsquo;s a <strong>particular instance of</strong> <a href="https://arxiv.org/abs/1611.08402">MoNet</a></li>
<li><strong>Future Improvement</strong>: <ul>
<li>handle large batch sizes</li>
<li>perform a thorough analysis on the model interpretability</li>
<li>extending the method to perform <strong>graph calssification</strong></li>
<li>extending the model to incorporate <strong>edge features</strong> (indicating replationship among nodes)</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr />
<p>(3) <code>Nov 2015</code> <a href="https://arxiv.org/abs/1511.05493">GATED GRAPH SEQUENCE NEURAL NETWORKS</a> <em>[Yujia Li, Daniel Tarlow, Marc Brockschmidt, Richard Zemel]</em></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p>Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study <strong>feature learning</strong> techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.</p>
</blockquote>
<hr />
<p>(4) <code>Feb 2017</code> <a href="https://arxiv.org/abs/1609.02907">Semi-Supervised Classification with Graph Convolutional Networks</a> <em>[Thomas N. Kipf, Max Welling]</em></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p>We present a <strong>scalable approach</strong> for <strong>semi-supervised learning</strong> on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a <strong>localized first-order approximation</strong> of spectral graph convolutions. Our model scales linearly in the number of <strong>graph edges</strong> and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on <strong>citation networks</strong> and on a <strong>knowledge graph dataset</strong> we demonstrate that our approach outperforms related methods by a significant margin.</p>
</blockquote>
<ul>
<li><strong>Key notes</strong>: <ul>
<li><u><strong>Main contributions</strong></u>: <ul>
<li><strong>[1]</strong> Direct encode the graph structure using a <u>neural network model</u> and train on a supervised target -&gt; <strong>avoding expilicit graph-based regularization in the loss function</strong></li>
<li><strong>[2]</strong> introduce a layer-wise propagation rule for NN models operates directly on graph -&gt; <u>motivated from a first order (linear) approximation of <strong>spectral graph convolutions</strong></u><ul>
<li>use <strong>renormalization trick</strong> to avoid numerical instabilities (gradient exploding/vanishing) while deep learning</li>
</ul>
</li>
<li><strong>[3]</strong> demonstrate such model can be used for <strong>fast and scalable</strong> semi-supervised classification <ul>
<li>We can condition the model $f(X,A)$ both on the data $X$ and adjacency matrix $A$ [<em>especially for scenarios where A contain information nnot present in X</em>]</li>
</ul>
</li>
<li><strong>[4]</strong> The author&rsquo;s model use a single weigh matrix per layer and deals with varying node degrees through an <strong>appropriate normalization of the adjacency matrix</strong></li>
<li>In this work, the author implicitly assumes: <ul>
<li><strong>locality</strong> (dependence on the kth-order neighborhood for a GCN with K layers)</li>
<li><strong>equal importance</strong> of <u>self-connections</u> vs. <u>edges to neighboring nodes</u></li>
</ul>
</li>
</ul>
</li>
<li><u><strong>Other Notes</strong></u>:<ul>
<li>In <strong>graph-based semi-supervised learning</strong>: label is smoothed via explicit graph-based <strong>regularization</strong><blockquote>
<p>like using graph Laplacian regularization in the loss fucntion</p>
</blockquote>
</li>
<li>graph representation for semi-supervised learning: <ul>
<li><strong>[i]</strong> use graph Laplacian regularization <code>label propagation</code> <code>manifold regularization</code> <code>deep semi-supervised embedding</code></li>
<li><strong>[ii]</strong> use graph embedding-based approches</li>
</ul>
</li>
</ul>
</li>
<li><u><strong>Use cases</strong></u>:<ul>
<li>on citation networks</li>
<li>on knowledge graph dataset</li>
</ul>
</li>
<li><u><strong>Further directions</strong></u>:<ul>
<li><strong>[1]</strong> Memory requirement (for very large and densely connected graph datasets, further approximations might be necessary)</li>
<li><strong>[2]</strong> directed edges and edge features (<u>not supported in this work</u>)</li>
<li><strong>[3]</strong> limiting assumptions<ul>
<li>
<blockquote>
<p><strong>introduce a trade-off parameter $\lambda$</strong> in the definition of $A$ (can be learned by GD)</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr />
<p>(5) <code>Oct 2017</code> <a href="https://arxiv.org/abs/1710.09599">Watch Your Step: Learning Node Embeddings via Graph Attention</a> <em>[Sami Abu-El-Haija, Bryan Perozzi, Rami Al-Rfou, Alex Alemi]</em></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p>Graph embedding methods represent nodes in a continuous vector space, preserving information from the graph (e.g. by sampling random walks). There are many hyper-parameters to these methods (such as random walk length) which have to be manually tuned for every graph. In this paper, we replace random walk hyper-parameters with <strong>trainable parameters that we automatically learn via backpropagation</strong>. In particular, we learn a novel attention model on the power series of the transition matrix, which guides the random walk to optimize an upstream objective. Unlike previous approaches to attention models, the method that we propose <strong>utilizes attention parameters exclusively on the data</strong> (e.g. on the random walk), and <strong>ot used by the model for inference</strong>. We experiment on link prediction tasks, as we aim to produce embeddings that best-preserve the graph structure, generalizing to unseen information. We improve state-of-the-art on a comprehensive suite of real world datasets including social, collaboration, and biological networks. <strong>Adding attention to random walks</strong> can reduce the error by 20% to 45% on datasets we attempted. Further, our learned attention parameters are different for every graph, and our automatically-found values agree with the optimal choice of hyper-parameter if we manually tune existing methods.</p>
</blockquote>
<ul>
<li><strong>Key notes</strong>: <ul>
<li><strong>Unsupervised graph embedding methods</strong>: <ul>
<li><strong>[i]</strong> sample pair-wise relationships from the graph through <strong>random walks</strong> and <strong>counting node co-occurance</strong></li>
<li><strong>[ii]</strong> train an embedding model (using <em>skipgram of word2vec</em>) to learn representations that encode pairwise node similarities</li>
<li><strong>[Problem]</strong>: significantly depend on hyper-parameters (e.g. length of random walk)</li>
</ul>
</li>
<li>In this work, the author replace the hyper-parameter (<code>C</code> for <em>length of random walk</em>; <code>Q</code> for <em>context distribution</em>) with <strong>trainable</strong> parameters: (-&gt; automatically learned for each graph)<ul>
<li>pose <strong>graph embedding</strong> as end2end learning (the above discrete 2 steps random walk co-occurance sampling)</li>
<li>followed by <strong>representation learning</strong>: <u>joint using a closed-form expectation over the graph adjacency matrix</u></li>
</ul>
</li>
<li><strong>contraibution</strong> summary: <ul>
<li><strong>[1]</strong> propose extendible family of graph attention models -&gt; <strong>learn arbitrary context distribution</strong></li>
<li><strong>[2]</strong> show optimal hyper-parameter (found by manual tunining) agrees</li>
<li><strong>[3]</strong> evaluate a number of challenging link prediction tasks</li>
</ul>
</li>
<li><u>Preliminaries</u>:<ul>
<li>Graph embedding </li>
<li><strong>Learning Embeddings via Random Walks</strong>: use w2v on path sequence from random walk</li>
<li>Graph Likelihood</li>
</ul>
</li>
<li>The authors extend the <strong>Negative Log Graph Likelihood</strong> loss to include <strong>attention parameters</strong> on the random walk sampling: <ul>
<li>Expectation on the co-occurance matrix: $E[D]$ to approximate</li>
<li><a href="http://www.perozzi.net/publications/14_kdd_deepwalk.pdf">DeepWalk</a> do not use $C$ (length of random walk) as a hard limit</li>
<li>try to learn the context distribution $Q$ (with <em>C-dimentional</em>)</li>
<li>train <strong>softmax attentio model</strong> on the <strong>infinite power series</strong> of the transition matrix</li>
</ul>
</li>
<li><strong>Extension</strong>: extend the model to learn the weight of any other type of pair-wise node similarity</li>
<li>learn a free-form contexts distribution with a parameter for each type of context similarity (distance in a random walk)</li>
</ul>
</li>
</ul>
<hr />
<p>(6) <code>Jun 2017</code> <a href="https://arxiv.org/abs/1706.02216">Inductive Representation Learning on Large Graphs</a> <em>[William L. Hamilton, Rex Ying, Jure Leskovec]</em> <code>MoNet</code></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p><strong>Low-dimensional</strong> embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are <u>inherently transductive</u> and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, <strong>inductive framework</strong> that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. <u>Instead of training individual embeddings for each node, we learn a function that generates embeddings by <strong>sampling and aggregating features</strong> from a node&rsquo;s local neighborhood</u>. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.</p>
</blockquote>
<hr />
<p>(7) <code>Jul 2015</code> <a href="https://arxiv.org/abs/1507.00280">Network Lasso: Clustering and Optimization in Large Graphs</a> <em>[David Hallac, Jure Leskovec, Stephen Boyd]</em></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p>Convex optimization is an essential tool for modern data analysis, as it provides a framework to formulate and solve many problems in machine learning and data mining. However, general convex optimization solvers do not scale well, and scalable solvers are often specialized to only work on a narrow class of problems. Therefore, there is a need for simple, scalable algorithms that can solve many common optimization problems. In this paper, we introduce the <strong>network lasso</strong>, a generalization of the group lasso to a network setting that allows for simultaneous clustering and optimization on graphs. We develop an algorithm based on the <strong>Alternating Direction Method of Multipliers (ADMM)</strong> to solve this problem in a distributed and scalable manner, which allows for guaranteed global convergence even on large graphs. We also examine a non-convex extension of this approach. We then demonstrate that many types of problems can be expressed in our framework. We focus on three in particular - binary classification, predicting housing prices, and event detection in time series data - comparing the network lasso to baseline approaches and showing that it is both a fast and accurate method of solving large optimization problems.</p>
</blockquote>
<ul>
<li><strong>Key notes</strong>: <ul>
<li><u><strong>Main contributions</strong></u>: <ul>
<li><strong>[i]</strong> formally define <strong>network lasso</strong>, a generalization of the group lasso to a network setting for simultaneous <strong>clustering</strong> &amp; <strong>optimization</strong> on graph<ul>
<li>network lasso problem: <u><strong>cost of node</strong> + <strong>edge cost</strong> (sum of norms of differences of the adjacent edge varialbes)</u> <em>e.g. each vertex might represent the action of a control system</em></li>
<li>the <strong>edge</strong> term are <u>regularization that encourages adjacent nodes to have close model parameters</u> -&gt; <code>adjacent nodes should have similar models</code></li>
</ul>
</li>
<li><strong>[ii]</strong> The author&rsquo;s <strong>distributed</strong> &amp; <strong>scalable</strong> solution -&gt; each vertex is controlled by one <code>agent</code>, they exchange information on the graph to solve the problem iteratively:<ul>
<li>propose a easy to implement algorithm based on <strong>Alternating Direction Method of Multipliers (ADMM)</strong></li>
</ul>
</li>
<li><strong>[iii]</strong> real examples</li>
</ul>
</li>
<li><u><strong>Other Notes</strong></u>:<ul>
<li>With <strong>large dataset</strong> classical methods of convex methods fail due to <strong>lack of scalability</strong> -&gt; <u>large-scale optimization</u> (<em>challenge: generalize, capable of scaling</em>)</li>
<li><strong>convex clustering</strong>: a well studied instance of network lasso</li>
</ul>
</li>
<li><u><strong>Use cases</strong></u>:<ul>
<li>problem of predicting house price<blockquote>
<p>The network lasso solution empirically determines the neighbourhoods, so that each house can share a common model with houses in its cluster</p>
</blockquote>
</li>
</ul>
</li>
<li><u><strong>Further directions</strong></u>:<ul>
<li>the analysis of different non-convex functions phi</li>
<li>many ways to inmprove speed, performan and robustness: <ol>
<li><em>find closed-form solution for common objective function f(x)</em></li>
<li><em>automatically determining the optimal ADMM parameter rho</em></li>
<li><em>allow edge objective function beyond just the weighted network lasso</em></li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr />
<p>(8) <code>May 2019</code> <a href="https://arxiv.org/abs/1905.10224">Semi-Supervised Classification on Non-Sparse Graphs Using Low-Rank Graph Convolutional Networks</a> <em>[Dominik Alfke, Martin Stoll]</em></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p>Graph Convolutional Networks (GCNs) have proven to be successful tools for semi-supervised learning on graph-based datasets. For <strong>sparse</strong> graphs, <em>linear and polynomial</em> filter functions have yielded impressive results. For <strong>large non-sparse graphs</strong>, however, network training and evaluation becomes prohibitively expensive. By introducing <strong>low-rank filters</strong>, we gain significant runtime acceleration and simultaneously improved accuracy. We further propose an architecture change mimicking techniques from Model Order Reduction in what we call a <strong>reduced-order GCN</strong>. Moreover, we present how our method can also be applied to <strong>hypergraph datasets</strong> and how <strong>hypergraph convolution</strong> can be implemented efficiently.</p>
</blockquote>
<ul>
<li><strong>Key notes</strong>: <ul>
<li><u><strong>Main contributions</strong></u>: <ul>
<li><strong>[1]</strong> introduction of <strong>low-rank filters</strong> -&gt; decrease runtimes and produce more accurate classification<ul>
<li>inspired by <u>Model Order Reduction</u> to accelerate the convolution operation</li>
<li>advantages of low-rank kernel matrices $K$: <ul>
<li><em>[i]</em> the matrix products are much cheaper to evaluate</li>
<li><em>[ii]</em> setting up it requires only a small number of eigenpairs</li>
<li><em>[iii]</em> the dominant eigenvalues include clustering information -&gt; damping noise to zero</li>
</ul>
</li>
</ul>
</li>
<li><strong>[2]</strong> introduction of <strong>pseudoinverse filter</strong> -&gt; better than standard linear filter</li>
<li><strong>[3]</strong> <strong>reduced-order GCN</strong> -&gt; dependent on the dataset, have good performance in hypergraph<ul>
<li>define the <strong>graph Laplacian</strong> in hypergraph</li>
</ul>
</li>
</ul>
</li>
<li><u><strong>Other Notes</strong></u>:<ul>
<li>Effect method for semi-supervised learning (<strong>SSL</strong>): <ul>
<li>
<blockquote>
<p><strong>a small set of training data</strong> and <strong>clustering information</strong> extracted from a vast amount of unlabeled data</p>
</blockquote>
</li>
</ul>
</li>
<li><strong>filter function space</strong> is a crucial design choice in each GCN architecture</li>
<li><strong>[clustering]</strong>: $i$-th  and $j$-th components of the vector are similar iff nodes $i$ and $j$ have a strong connection in the dataset graph</li>
</ul>
</li>
<li><u><strong>Use cases</strong></u>:<ul>
<li>on <strong>Hypergraph</strong>, i.e. <em>[containing categorical data]</em></li>
<li>good for data point classification</li>
</ul>
</li>
<li><u><strong>Further directions</strong></u>:</li>
</ul>
</li>
</ul>
<hr />
<p>(9) <code>Jun 2017</code> <a href="https://arxiv.org/abs/1706.02216">Inductive Representation Learning on Large Graphs</a> <em>[William L. Hamilton, Rex Ying, Jure Leskovec]</em> <code>Negative sampling</code></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p>Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node&rsquo;s local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.</p>
</blockquote>
<ul>
<li><strong>Key notes</strong>: <ul>
<li></li>
</ul>
</li>
</ul>
<hr />
<p>(10) <code>May 2017</code> <a href="https://arxiv.org/abs/1705.05615">Learning Edge Representations via Low-Rank Asymmetric Projections</a> <em>[Sami Abu-El-Haija, Bryan Perozzi, Rami Al-Rfou]</em> <code>Graph likelihood</code></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p>We propose a new method for <strong>embedding graphs</strong> while preserving <strong>directed</strong> edge information. Learning such continuous-space vector representations (or embeddings) of nodes in a graph is an important first step for using network information (from social networks, user-item graphs, knowledge bases, etc.) in many machine learning tasks.
Unlike previous work, we </p>
</blockquote>
<ul>
<li>
<blockquote>
<p>(1) explicitly model an edge as a function of node embeddings</p>
</blockquote>
</li>
<li>
<blockquote>
<p>(2) propose a novel objective, the &ldquo;<strong>graph likelihood</strong>&rdquo;, which contrasts information from sampled random walks with non-existent edges. </p>
</blockquote>
</li>
</ul>
<blockquote>
<p>Individually, both of these contributions improve the learned representations, especially when there are memory constraints on the total size of the embeddings. When combined, our contributions enable us to significantly improve the state-of-the-art by learning more concise representations that better preserve the graph structure.
We evaluate our method on a variety of <strong>link-prediction</strong> task including social networks, collaboration networks, and protein interactions, showing that our proposed method learn representations with error reductions of up to 76% and 55%, on directed and undirected graphs. In addition, we show that the representations learned by our method are quite space efficient, producing embeddings which have higher structure-preserving accuracy but are 10 times smaller.</p>
</blockquote>
<ul>
<li><strong>Key notes</strong>: <ul>
<li><u><strong>Main contributions</strong></u>: <ul>
<li><strong>[1]</strong> explicitly model a <strong>directed edge function</strong> -&gt; <strong>[low-rank affine projects]</strong> on a manifold that is produced by a DNN (<code>use DNN to map nodes onto a low-dimensional manifold</code>) + (<code>define a function between two nodes as a projection in the manifold coordinates</code>)<ul>
<li><strong>[i]</strong> $f()$ removes <u>degrees of freedom</u> as it passes an embedding through the DNN activation functions </li>
<li><strong>[ii]</strong> reduces overfitting and improve generalization (<em>as f can contrain the embeddings with less degrees of freedom</em>)</li>
<li><strong>[iii]</strong> hidden layers inf find correlations in the data, as many graphnodes have similar connections</li>
</ul>
</li>
<li><strong>[2]</strong> propose a new objective function, <strong>the graph likelihood</strong> -&gt; <u>joinly maximizing the edge function and the manifold</u> (<em>inspired from MLE in logistic regression</em>)</li>
<li><strong>[3]</strong> improve the SOTA on learning continuous graph representation, especailly on <strong>directed graphs</strong> while producing significantly <strong>smaller</strong> representation</li>
<li>learn an asymmetric transformation of the nodes -&gt; combined for any pair of nodes to moedel the strength of their directed relationships</li>
<li>Adjacency (non-embedding) Baselines: <code>Jaccard Coefficient</code>, <code>Common Neighbors</code>, <code>Adamic Adar</code></li>
<li>Embedding methods: <code>Laplacian EigenMaps</code>, <code>node2vec</code>, <code>DNGR</code></li>
</ul>
</li>
<li><u><strong>Other Notes</strong></u>:<ul>
<li><u>continuous space representations</u>: leanr a vector space that highly preserve the graph structure</li>
<li>traditional <strong><em>eigen</em></strong> methods learn embeddings that <code>minimize the euclidean distance of the connected nodes</code></li>
<li>shortcomings of embedding method (like random wlak):<ul>
<li>do not explictly model deges</li>
<li>unable to capture asymmetic relationships (different direction of edges)</li>
</ul>
</li>
</ul>
</li>
<li><u><strong>Use cases</strong></u>:<ul>
<li><strong>link-prediction</strong> in <code>social network</code>, <code>collaboration network</code> and <code>protein interaction</code></li>
<li><em>followers</em> and <em>followees</em> </li>
</ul>
</li>
<li><u><strong>Further directions</strong></u>:<ul>
<li>further investigation for learning continuous representation of graphs</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr />
<p>(11) <code>Mar 2014</code> <a href="https://arxiv.org/abs/1403.6652">DeepWalk: Online Learning of Social Representations</a> <em>[Bryan Perozzi, Rami Al-Rfou, Steven Skiena]</em> <code>DeepWalk</code> </p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p>We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk&rsquo;s latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk&rsquo;s representations can provide F1 scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk&rsquo;s representations are able to outperform all baseline methods while using 60% less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.</p>
</blockquote>
<ul>
<li><strong>Key notes</strong>: <ul>
<li></li>
</ul>
</li>
</ul>
<hr />
<p>(12) <code>Aug 2018</code> <a href="https://arxiv.org/abs/1808.02590">A Tutorial on Network Embeddings</a> <em>[Haochen Chen, Bryan Perozzi, Rami Al-Rfou, Steven Skiena]</em></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p>Network embedding methods aim at learning <strong>low-dimensional latent representation</strong> of nodes in a network. These representations can be used as features for a wide range of tasks on graphs such as classification, clustering, link prediction, and visualization. In this survey, we give an overview of network embeddings by summarizing and categorizing recent advancements in this research field. 
We first discuss the <strong>[1] desirable properties</strong> of network embeddings and briefly introduce the <strong>[2] history</strong> of network embedding algorithms. Then, we discuss network embedding methods under <strong>[3]different scenarios</strong>, such as <em>supervised versus unsupervised learning</em>, <em>learning embeddings for <strong>homogeneous</strong> networks versus for <strong>heterogeneous</strong> networks</em>, etc. We further demonstrate the <strong>[4]applications</strong> of network embeddings, and conclude the survey with <strong>[5]future work</strong> in this area.</p>
</blockquote>
<ul>
<li>
<p><strong>Key notes</strong>: </p>
<ul>
<li>
<p><u><strong>Main contributions</strong></u>: </p>
<ul>
<li><strong>Brief history of Network Embedding</strong>: (lower performance compared to <strong>Deep Learning methods</strong>)<ul>
<li><strong>[1] PCA and Multidimensional Scaling (MDS)</strong>: &lt;$O(n^3)$&gt;<ul>
<li>
<blockquote>
<p><strong>MDS</strong>: project each row of M to a k-dim components -&gt; the distance between different objects in the oringinal feature matrix M is best preserved in the k-dim space</p>
</blockquote>
</li>
<li>The matrix to factorize could be <code>adjacency matrix</code>, <code>normalized Laplacian matrix</code>, <code>all-pairs shortest path matrix</code></li>
<li>Both PCA &amp; MDS <strong>fail to discover the non-linearity</strong> and <strong>have high time complexity</strong></li>
</ul>
</li>
<li><strong>[2] IsoMap and Locally Linear Embeddings (LLE)</strong>: -&gt; <strong>non-linear</strong><ul>
<li>
<blockquote>
<p><strong>IsoMap</strong>: extension of MDS, <em>preserving geodesic distances in the neighborhood graph of input data</em> (neighborhood of <strong>node i</strong> contructed by connecting [nodes closer than a threshold]/[nodes which are k-nearest neighbors])</p>
</blockquote>
</li>
<li>
<blockquote>
<p><strong>LLE</strong>: only exploits the local neighborhood of data point, not estimate distance between distant data points</p>
</blockquote>
</li>
<li><strong>time complexity</strong> to large</li>
</ul>
</li>
<li><strong>[3] Laplacian eigenmaps (LE)</strong>: -&gt; use <strong>spectral properties (eigenvectors)</strong><ul>
<li>
<blockquote>
<p><strong>LE</strong>: represent each node by [eigenvectors associated with its <strong>k-smallest</strong> nontrivial eigenvalues]</p>
</blockquote>
</li>
</ul>
</li>
<li><strong>[4] SocDim</strong>: using the spectral properties of the <strong>[modularity matrix]</strong> as latent social dim in the network</li>
</ul>
</li>
<li><strong>With Deep Learning method</strong>: <strong>Deep walk</strong><ul>
<li>
<blockquote>
<p><strong>Advantanges</strong>: [1] can be generated on demand; [2]scalable <strong>[3] intraoduce a paradigm for deep learning on graphs</strong></p>
</blockquote>
</li>
<li><u>Deep Walk Paradigm</u>: can be expanded in <code>complexity of graphs</code>, <code>complexity of methods</code><ul>
<li><strong>[i]</strong> Choose a matrix associated with the input graph</li>
<li><strong>[ii]</strong> <strong>Graph Sampling</strong>: Sample sequences from the chose matrix <ol>
<li>save time by approximating the matrix</li>
<li>sequence of symbols are much more easier for deep learning models to deal with</li>
</ol>
</li>
<li><strong>[iii]</strong> Learn embeddings from the sequences or the matrix itself</li>
<li><strong>[iv]</strong> ouptut node embeddings</li>
</ul>
</li>
</ul>
</li>
<li><strong>Unsupervised</strong> Network Embedding:<ul>
<li><u>Summary of unsupervised network embedding methods</u> <strong>[in simple undirected graphs]</strong><ul>
<li><code>Deep Walk</code>, <code>LINE</code>, <code>Node2vec</code>, <code>Walklets</code>, <code>GraRep</code> -&gt; all have hyper-parameter</li>
<li><code>GraphAttention</code> -&gt; learn the attention over the power series of the graph transition matrix <ul>
<li>
<blockquote>
<p>learns a multi-scale representation which best predicts links in the original graph</p>
</blockquote>
</li>
</ul>
</li>
<li><code>SDNE</code>, <code>DNGR</code> -&gt; combined with autoencoder</li>
</ul>
</li>
<li><u>Directed Graph Embedding</u>: <code>HOPE</code> <a href="https://arxiv.org/abs/1705.05615">Learning Edge Representations via Low-Rank Asymmetric Projections</a></li>
<li><u>Edge Embeddings</u>: -&gt; <code>link prediction</code> <a href="https://arxiv.org/abs/1705.05615">Learning Edge Representations via Low-Rank Asymmetric Projections</a><ul>
<li>
<blockquote>
<p>learn edge representations via low-rank asymmetric projections</p>
</blockquote>
</li>
</ul>
</li>
<li><u>Signed Graph Embeddings</u>: <code>SiNE</code> &amp; <code>SNE</code><ul>
<li>
<blockquote>
<p><strong>SiNE</strong>: maximizing the margin between the embedding similarity of friends (+1 connected) and the embedding similarity of foes (-1 connected)</p>
</blockquote>
</li>
<li>
<blockquote>
<p><strong>SNE</strong>: predicts the representation of a target node by linearly combines the representation of its context nodes; [tow signed-type vector are incorporated into the log-bilinear model]</p>
</blockquote>
</li>
</ul>
</li>
<li><u>Subgraph Embeddings</u>: <ul>
<li><code>Deep graph kernel</code>: <em>general framework for modelling sub-structure similarity in graphs</em></li>
</ul>
</li>
<li><u>Meta-strategies for Improving NEtwork Embeddings</u><ul>
<li>[weakness of neural methods for network embeddings]:<ol>
<li>all <strong>local approaches</strong> -&gt; limited to strucuture immediately around a node, <strong>fail to uncover important long distance global structural pattern</strong></li>
<li>all rely on <strong>non-convex optimization goal</strong></li>
</ol>
</li>
<li><a href="https://arxiv.org/abs/1706.07845">HARP</a>: <code>embedding capture both the local and global structures of the ven graphs</code></li>
</ul>
</li>
</ul>
</li>
<li><strong>Attributed Network Embedding</strong>: Desirable to learn from <code>node attributes</code> and <code>edge attributes</code><ul>
<li><strong>[1]</strong> textual attrigutes<ul>
<li>
<blockquote>
<p><strong>TADW</strong>: incorporates the text features into the matrix factorization process</p>
</blockquote>
</li>
<li>Jointly model network structure and node features -&gt; <code>enforce the embedding similarity between nodes with similar feature vectors</code><ul>
<li>
<blockquote>
<p><strong>CENE</strong>: treats text content as a special type of node -&gt; [node-node links] &amp; [node-content links] for node embedding</p>
</blockquote>
</li>
</ul>
</li>
<li>
<blockquote>
<p><strong>HSCA</strong></p>
</blockquote>
</li>
</ul>
</li>
<li><strong>[2]</strong> node labels: e.g. <code>citation network</code>: <em>venue</em> or <em>year of publication</em><ul>
<li>
<blockquote>
<p><strong>GENE</strong>: also predicts the group information of context nodes as a part of the optimization goal</p>
</blockquote>
</li>
<li><a href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14589">Community Preserving Network Embedding</a>: preserves the <strong>community structures within network</strong></li>
</ul>
</li>
<li><strong>[3]</strong> semi-supervised network embedding methods: <ul>
<li><a href="https://arxiv.org/abs/1603.08861">Planetoid</a></li>
<li><a href="https://www.ijcai.org/Proceedings/16/Papers/547.pdf">Max-margin DeepWalk (MMDW)</a></li>
</ul>
</li>
</ul>
</li>
<li><strong>Heterogeneous Network Embedding</strong> <em>(jointly minimizing the loss over each modality)</em><ul>
<li><a href="http://www.ifp.illinois.edu/~chang87/papers/kdd_2015.pdf">Heterogeneous Network Embedding via Deep Architectures</a>: <code>feature representation for each modality</code> -&gt; <code>map them to same embedding spae</code></li>
<li><a href="https://pdfs.semanticscholar.org/ff66/50ee2efab6f6ec155ecb644a329397cb16fa.pdf">Representation Learning for Measuring Entity Relatedness with Rich Information</a></li>
<li><a href="https://arxiv.org/abs/1510.05198">Learning multi-faceted representations of individuals from heterogeneous evidence using neural networks</a> -&gt; neural network model for learning user representation in a heterogeneous social network</li>
<li><strong>HEBE</strong>:  embedding large-scale heterogeneous event network</li>
<li><strong>EOE</strong>: for coupled heterogeneous network (two networks connected by inter-network edges)</li>
<li><strong>Metapath2vec</strong>: -&gt; [extending random walks and embeding learning methods to hetero-networks]</li>
</ul>
</li>
</ul>
</li>
<li>
<p><u><strong>Other Notes</strong></u>:</p>
<ul>
<li>due to <strong>[billions of nodes and edges in the information network can be intractable to perform complex inference procedures]</strong> -&gt; use <strong>network embedding</strong> to solve this problem<ul>
<li>
<blockquote>
<p>find a mapping function: [converts each node in the nework to a low-dimenstional latent representation] (can be use as features)</p>
</blockquote>
</li>
</ul>
</li>
<li><u>Target network embedding characteristics</u>: <ul>
<li><strong>[1] Adaptability</strong>: <em>new application should not require repeating learning process</em></li>
<li><strong>[2] Scalability</strong>: <em>able to process large-scale networks in a <u>short period of time</u></em></li>
<li><strong>[3] Community aware</strong>: <em>distance between latent representations should represent a <strong>metric</strong> for evaluating <strong>similarity</strong> between the corresponding members of the network</em> -&gt; <code>generalization in networks with homophily</code></li>
<li><strong>[4] Low dimensional</strong>: -&gt; better and speed up convergence and inference</li>
<li><strong>[5] Continuous</strong>: to model partial community membership in continuous space, <u>continuous representation has smooth decision boundatires between communities</u> -&gt; <strong>Robust Classification</strong></li>
</ul>
</li>
<li><strong>Heterogeneous Network</strong>: <code>network with multiple type of nodes or multiple types of edges</code></li>
<li><strong>Signed Graph</strong>: <code>edge is assigned with a weight from {+1,-1}</code> -&gt; could be used to reflect <strong>agreement or trust</strong></li>
</ul>
</li>
<li>
<p><u><strong>Use cases and application</strong></u>:</p>
<ul>
<li><u>Knowledge Representation</u>: <em>[encoding facts about the world using short sentences composed of (subjects, predicates and objects)]</em><ul>
<li><code>GenVector</code>: learning social knowledge graphs</li>
<li><code>RDF2Vec</code>(Resource Description Framework)</li>
</ul>
</li>
<li><u>Recommender Systems</u>:<ul>
<li>
<blockquote>
<p>interactins between (<strong>users</strong>, <strong>users&rsquo; queries</strong>, <strong>items</strong>) -&gt; form a heterogeneous network to encode the latent preference of users over time</p>
</blockquote>
</li>
<li><a href="https://dl.acm.org/citation.cfm?id=2959169">Query-based Music Recommendations via Preference Embedding</a> -&gt; embed user preference and query intention into low-dimensional vector space</li>
</ul>
</li>
<li><u>NLP</u>:<ul>
<li><code>PLE</code> (label noise reduction in entity typing), <code>CANE</code>(context-aware netowrk embedding frame work), <code>community-based QA framework</code></li>
</ul>
</li>
</ul>
</li>
<li>
<p><u>Social Network Analysis</u> <code>refer back to the paper</code></p>
<ul>
<li><em>predicting the exact age of users in social network</em></li>
<li>modelling social network and mobile trajectories simultaneously</li>
<li>Obtain conect embedding of higher quality, by learning <strong>wikipedia page representations</strong></li>
<li>align usersacross different social network</li>
<li>measuring similarity between historical figures</li>
<li>browsing through large lists in the absense of a predefined hierachy</li>
</ul>
</li>
<li>
<p><u><strong>Further directions</strong></u>:</p>
<ul>
<li><strong>[Problem 1]</strong>: most of the strategies relies on a <strong>rigid definition of context nodes indentical for all networks</strong> -&gt; <code>unifying different network embedding under a general framework</code>, only <strong>Graph Attention</strong> have compacity for different networks</li>
<li><strong>[Problem 2]</strong>: dependence upon general loss function and optimization models, suboptimal compared to <code>end2end embeddings methods designed specifically for a task</code><ul>
<li>
<blockquote>
<p>design loss functions and optimization models for a specific task</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr />
<p>(13) <code>Feb 2018</code> <a href="https://arxiv.org/abs/1802.09691">Link Prediction Based on Graph Neural Networks</a> <em>[Muhan Zhang, Yixin Chen]</em></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p><strong>Link prediction</strong> is a key problem for network-structured data. Link prediction heuristics use some score functions, such as <strong>common neighbors</strong> and <strong>Katz index</strong>, to measure the <strong>likelihood</strong> of links. They have obtained wide practical uses due to their <strong>simplicity, interpretability, and for some of them, scalability</strong>. However, every heuristic has a <strong>strong assumption</strong> on <em>when two nodes are likely to link</em>, which limits their effectiveness on networks where these assumptions fail. In this regard, a more reasonable way should be learning a suitable heuristic from a given network instead of using predefined ones. By extracting a <strong>local subgraph</strong> around each target link, we aim to learn a function mapping the subgraph patterns to link existence, thus automatically learning a heuristic&rsquo; that suits the current network. In this paper, we study this heuristic learning paradigm for link prediction. </p>
<p><strong>[1]</strong>First, we develop a novel <strong>γ-decaying heuristic theory</strong>. The theory unifies a wide range of heuristics in a single framework, and proves that all these heuristics can be well approximated from local subgraphs. Our results show that local subgraphs reserve rich information related to link existence. </p>
<p><strong>[2]</strong>Second, based on the γ-decaying theory, we propose a new algorithm to learn heuristics from local subgraphs using a graph neural network (GNN). Its experimental results show unprecedented performance, working consistently well on a wide range of problems.</p>
</blockquote>
<ul>
<li>
<p><strong>Key notes</strong>: </p>
<ul>
<li>
<p><u><strong>Other Notes</strong></u>:</p>
<ul>
<li><strong>[link prediction]</strong>: predict whether two nodes in a network are likely to have a link</li>
<li><strong>[Heuristic methods]</strong>: compute some heuristic node similarity scores as the likelihood of links -&gt; <em>can be categorized based on the <u>maximum hop of neighbors</u></em><ul>
<li><strong>First-order heuristics</strong>: <code>common neighbors (CN)</code>, <code>preferential attachment (PA)</code></li>
<li><strong>Second-order heuristics</strong>: <code>Adamic-Adar (AA)</code>, <code>Resource allocation (RA)</code></li>
<li><strong>[h-order heuristics]</strong>: which require knowing up to h-hop neighborhood of the target nodes</li>
<li>
<blockquote>
<p>some <strong>high order heuristics</strong> require knowing the entire network e.g. <code>Katz</code>, <code>PageRank (PR)</code>, <code>SimRank (SR)</code></p>
</blockquote>
</li>
</ul>
</li>
<li>Heuristic methods have <strong>strong assumptions</strong> on when links may exist e.g. <code>two nodes are more likely to connect if they have many common neighbors</code> -&gt; <u>might cause problem if the assumption fail in specific network</u></li>
<li><strong>Graph structure features (include heuristics)</strong>: features located inside the observed node and edge structure of the network -&gt; can be calculated directly from the graph</li>
<li><a href="https://www.cse.wustl.edu/~muhan/papers/KDD_2017.pdf">Weisfeiler-Lehman Neural Machine (WLNM)</a>: use <u>fully connected neural network</u> to learn <u>which enclosing subgraphs correspond to link existence</u> - learn heuristic automatically <ul>
<li>
<blockquote>
<p><strong>problem 1</strong>: high order heuristic have much better performance but need entire network as subgraph -&gt; which means unaffordable time and memory consumption</p>
</blockquote>
</li>
<li>
<blockquote>
<p><strong>problem 2</strong>: fully connected layer -&gt; fixed size tensors -&gt; information loss due to truncate</p>
</blockquote>
</li>
<li>
<blockquote>
<p><strong>problem 3</strong>: due to limitation of <strong>adjacency mnatrix representatinon</strong> -&gt; cannot combine latent &amp; explicit features</p>
</blockquote>
</li>
<li>
<blockquote>
<p><strong>problem 4</strong>: lack of theoretical justification</p>
</blockquote>
</li>
</ul>
</li>
<li><strong>Latent feature</strong>: use matrix representation of the network to learn low-dimentional representation/embedding for each node; <strong>explicit features</strong>: node attributes, describing all kinds of side information about individual nodes<ul>
<li>
<blockquote>
<p>combine both with graph structure feature could improve performance</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><u><strong>Main contributions</strong></u>: </p>
<ul>
<li><strong>[1]</strong> new theory (proof) for <strong>learning link prediction heuristics</strong> - <strong>learn from local enclosing subgraph</strong> <ul>
<li>using <strong>$\gamma$-decaying theory</strong> to effectively approximated from an h-hop enclosing subgraph</li>
<li>from it we are able to accurately calculate 1st and 2nd order heuristic, and approximate a wide range of high-order heuristics with small errors</li>
</ul>
</li>
<li><strong>[2]</strong> Novel link prediction framework <strong>SEAL</strong>: to learn general graph structure features (heuristics) from <strong>local</strong> enclosing subgraph as input<ul>
<li>Use <strong>GNN</strong> (graph convolusion layer) instead of <em>fully-connected NN</em></li>
<li>SEAL permits not only <strong>subgraph structures</strong> but aslo <strong>latent and explicit</strong> node features -&gt; refer to <u><strong><Other Notes></strong></u><ul>
<li>concatenate the features to $X$</li>
<li>use negative injection</li>
</ul>
</li>
<li>Include 3 steps: <ul>
<li><strong>[1]</strong> enclosing subgraph extraction for a set of sampled positive (observed) links and set of sampled negative (unobserved) links</li>
<li><strong>[2]</strong> node information matrix construction;</li>
<li><strong>[3]</strong> GNN learning -&gt; adjacency matrix + node information matrix</li>
</ul>
</li>
<li><u>node labelling</u> is important -&gt; <strong>let GNN tell where are the target nodes between which a link existence should be predicted</strong><ul>
<li>the authors propose a <strong>Double-Radius Node Labeling (DRNL)</strong>, it has a perfect <strong>hashing function</strong> -&gt; allows fast closed-form computations<blockquote>
<p>iteratively assign larger labels to nodes with a <strong>larger radius</strong> wrt both center nodes</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><u><strong>Use cases</strong></u>:</p>
<ol>
<li>friend recommendation</li>
<li>movie recommendation</li>
<li>knowledge graph completion</li>
<li>metabolic network reconstruction</li>
</ol>
</li>
<li>
<p><u><strong>Further directions</strong></u>:</p>
<ul>
<li>knowledge graph completion</li>
<li>recommender system</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr />
<p>(14) <code>Sep 2018</code> <a href="https://arxiv.org/abs/1809.10341">Deep Graph Infomax</a> <em>[Petar Veličković, William Fedus, William L. Hamilton, Pietro Liò, Yoshua Bengio, R Devon Hjelm]</em></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p>We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs&mdash;both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.</p>
</blockquote>
<ul>
<li><strong>Key notes</strong>: <ul>
<li></li>
</ul>
</li>
</ul>
<hr />
<p>(15) <code>Nov 2016</code> <a href="https://arxiv.org/abs/1611.07308">Variational Graph Auto-Encoders</a> <em>[Thomas N. Kipf, Max Welling]</em></p>
<p><a href="https://towardsdatascience.com/tutorial-on-variational-graph-auto-encoders-da9333281129"><em>Tutorial</em></a></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p>We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.</p>
</blockquote>
<ul>
<li><strong>Key notes</strong>: <ul>
<li><u><strong>Contributions</strong></u>: <ul>
<li>Introduced <strong>variational graph VAE</strong>: -&gt; <blockquote>
<p>learn interpretable latent representation for <strong>undirected</strong> graphs</p>
</blockquote>
<ul>
<li><strong>GCN encoder</strong> -&gt; <strong>inner product decoder</strong></li>
</ul>
</li>
<li>use model for link prediction task in citation network</li>
</ul>
</li>
<li><u><strong>Other notes</strong></u>:<ul>
<li>Adding input features significantly improves predictive perfomance across datasets</li>
</ul>
</li>
<li><u><strong>Further direction</strong></u>:<ul>
<li>investigate better-suited prior ditribution other than <strong>Gaussian prior</strong></li>
<li>more flexible generative models</li>
<li>the application of a stochastic gradient descent algorithm for improved scalability</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr />
<p>(16) <code>Apr 2019</code> <a href="https://arxiv.org/abs/1904.03751">DeepGCNs: Can GCNs Go as Deep as CNNs?</a> <em>[Guohao Li, Matthias Müller, Ali Thabet, Bernard Ghanem]</em></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p>Convolutional Neural Networks (CNNs) achieve impressive performance in a wide variety of fields. Their success benefited from a massive boost when very deep CNN models were able to be reliably trained. Despite their merits, CNNs fail to properly address problems with non-Euclidean data. To overcome this challenge, Graph Convolutional Networks (GCNs) build graphs to represent non-Euclidean data, borrow concepts from CNNs, and apply them in training. GCNs show promising results, but they are usually limited to very shallow models due to the vanishing gradient problem. As a result, most state-of-the-art GCN models are no deeper than 3 or 4 layers. In this work, we present new ways to successfully train very deep GCNs. We do this by borrowing concepts from CNNs, specifically residual/dense connections and dilated convolutions, and adapting them to GCN architectures. Extensive experiments show the positive effect of these deep GCN frameworks. Finally, we use these new concepts to build a very deep 56-layer GCN, and show how it significantly boosts performance (+3.7% mIoU over state-of-the-art) in the task of point cloud semantic segmentation. We believe that the community can greatly benefit from this work, as it opens up many opportunities for advancing GCN-based research.</p>
</blockquote>
<ul>
<li><strong>Key notes</strong>: <ul>
<li></li>
</ul>
</li>
</ul>
<hr />
<p>(17) <code>Jun 2019</code> <a href="https://arxiv.org/abs/1906.07159">vGraph: A Generative Model for Joint Community Detection and Node Representation Learning</a> <em>[Fan-Yun Sun, Meng Qu, Jordan Hoffmann, Chin-Wei Huang, Jian Tang]</em></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p>This paper focuses on two fundamental tasks of graph analysis: community detection and node representation learning, which capture the global and local structures of graphs, respectively. In the current literature, these two tasks are usually independently studied while they are actually highly correlated. We propose a probabilistic generative model called vGraph to learn community membership and node representation collaboratively. Specifically, we assume that each node can be represented as a mixture of communities, and each community is defined as a multinomial distribution over nodes. Both the mixing coefficients and the community distribution are parameterized by the low-dimensional representations of the nodes and communities. We designed an effective variational inference algorithm which regularizes the community membership of neighboring nodes to be similar in the latent space. Experimental results on multiple real-world graphs show that vGraph is very effective in both community detection and node representation learning, outperforming many competitive baselines in both tasks. We show that the framework of vGraph is quite flexible and can be easily extended to detect hierarchical communities.</p>
</blockquote>
<ul>
<li><strong>Key notes</strong>: <ul>
<li></li>
</ul>
</li>
</ul>
<hr />
<p>(18) <code>Nov 2018</code> <a href="https://arxiv.org/abs/1811.11103">Bayesian graph convolutional neural networks for semi-supervised classification</a> <em>[Yingxue Zhang, Soumyasundar Pal, Mark Coates, Deniz Üstebay]</em></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p>Recently, techniques for applying convolutional neural networks to graph-structured data have emerged. Graph convolutional neural networks (GCNNs) have been used to address node and graph classification and matrix completion. Although the performance has been impressive, the current implementations have limited capability to incorporate uncertainty in the graph structure. Almost all GCNNs process a graph as though it is a ground-truth depiction of the relationship between nodes, but often the graphs employed in applications are themselves derived from noisy data or modelling assumptions. Spurious edges may be included; other edges may be missing between nodes that have very strong relationships. In this paper we adopt a Bayesian approach, viewing the observed graph as a realization from a parametric family of random graphs. We then target inference of the joint posterior of the random graph parameters and the node (or graph) labels. We present the Bayesian GCNN framework and develop an iterative learning procedure for the case of assortative mixed-membership stochastic block models. We present the results of experiments that demonstrate that the Bayesian formulation can provide better performance when there are very few labels available during the training process.</p>
</blockquote>
<ul>
<li><strong>Key notes</strong>: <ul>
<li></li>
</ul>
</li>
</ul>
<hr />
<p>(19) <code>May 2017</code> <a href="https://arxiv.org/abs/1705.08415">Supervised Community Detection with Line Graph Neural Networks</a> <em>[Zhengdao Chen, Xiang Li, Joan Bruna]</em></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p>We study data-driven methods for community detection on graphs, an inverse problem that is typically solved in terms of the spectrum of certain operators or via posterior inference under certain probabilistic graphical models. Focusing on random graph families such as the stochastic block model, recent research has unified both approaches and identified both statistical and computational signal-to-noise detection thresholds. This graph inference task can be recast as a node-wise graph classification problem, and, as such, computational detection thresholds can be translated in terms of learning within appropriate models. We present a novel family of Graph Neural Networks (GNNs) and show that they can reach those detection thresholds in a purely data-driven manner without access to the underlying generative models, and even improve upon current computational thresholds in hard regimes. For that purpose, we propose to augment GNNs with the non-backtracking operator, defined on the line graph of edge adjacencies. We also perform the first analysis of optimization landscape on using GNNs to solve community detection problems, demonstrating that under certain simplifications and assumptions, the loss value at the local minima is close to the loss value at the global minimum/minima. Finally, the resulting model is also tested on real datasets, performing significantly better than previous models.</p>
</blockquote>
<ul>
<li><strong>Key notes</strong>: <ul>
<li></li>
</ul>
</li>
</ul>
<hr />
<p>(20) <code>Oct 2018</code> <a href="https://arxiv.org/abs/1810.05997">Predict then Propagate: Graph Neural Networks meet Personalized PageRank</a> <em>[Johannes Klicpera, Aleksandar Bojchevski, Stephan Günnemann]</em></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p><strong>Neural message passing</strong> algorithms for <strong>semi-supervised</strong> classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an <strong>improved propagation scheme</strong> based on <strong>personalized PageRank</strong>. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model&rsquo;s training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.</p>
</blockquote>
<ul>
<li>
<p><strong>Key notes</strong>: </p>
<ul>
<li><strong><u>Main Contribution</u></strong>:<ul>
<li>Highlight the inherent connectin between the <strong>limited distribution</strong> and <strong>PageRank</strong></li>
<li>Propose an algorithm -&gt; <u>utilizes a propagation scheme derived from <strong>personalized PageRank</strong></u><ul>
<li>add a chance of <strong>teleporting back to the root node</strong> -&gt; <strong>PageRank</strong> score <u>encodes the local neighborhood for every root node</u></li>
<li>
<blockquote>
<p>the teleport vector allow us to preserve the node;s local neighborhood even in the limit distribution</p>
</blockquote>
</li>
</ul>
</li>
<li>show the propagation scheme permits the use of far more propagation steps <strong>without lead to oversmoothing</strong></li>
<li><u>The algorithm separates the neural network from the propagation scheme</u> -&gt; achieve higher range without changing NN<ul>
<li>
<blockquote>
<p>decouples prediction and propagation and solves the limited range problem inherent in many message passing models without introducing any additional parameters</p>
</blockquote>
</li>
</ul>
</li>
<li><strong>independent</strong> development of the propagation algorithm and the nerual network generating predictions from node features</li>
<li>adding the propagation sheme during <strong>inference</strong> could significantly improves the accuracy without using any graph information</li>
</ul>
</li>
<li>
<p><strong><u>Other Notes</u></strong>:</p>
<ul>
<li>Deep Learning on Graph: <ul>
<li>node embedding (without node features, normally unsupervised)</li>
<li>use graph structure and node features (supervised)<ul>
<li><strong>[i]</strong> spectral GCN</li>
<li><strong>[ii]</strong> message passing</li>
<li><strong>[iii]</strong> neighbor aggregation via RNN: <em>very limited neighborhood for each node</em></li>
</ul>
</li>
<li><u>Increasing the size of the neighborhood</u> -&gt; Laplacian smoothing and too many layers leed to <strong>oversmoothing</strong></li>
</ul>
</li>
<li><code>solved by author in this paper</code><strong>[why GCN cannot be trivially expanded to use a larger neighborhood]</strong>:<ul>
<li><strong>[1]</strong> aggregation by averaging causes oversmoothing if <u>too many layers</u> </li>
<li><strong>[2]</strong> Most use learnable weight matrices in each layer -&gt; <strong>increases <u>depth</u> and <u>number</u> of parameters in large neighborhood</strong></li>
</ul>
</li>
<li>Differences between <strong>limited distribution</strong> &amp; <strong>PageRank</strong>: <ul>
<li>added self-loops</li>
<li>adjacency matrix normalization</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong><u>Use cases</u></strong>:</p>
</li>
<li>
<p><strong><u>Further direction</u></strong>:</p>
<ul>
<li>Combine PPNP with more complex neural networks used</li>
<li>faster or incremental approximations of personalized PageRank</li>
<li>More sophisticated propagation schemes </li>
</ul>
</li>
<li>
<p><strong><u>Important references</u></strong>:</p>
<ul>
<li>
<blockquote>
<p>Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, and Jie Tang. Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec. In ACM International Conference on Web Search and Data Mining (WSDM), 2018.</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr />
<p>(21) <code>May 2019</code> <a href="https://arxiv.org/abs/1905.10261">Approximation Ratios of Graph Neural Networks for Combinatorial Problems</a> <em>[Ryoma Sato, Makoto Yamada, Hisashi Kashima]</em></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p>In this paper, from a theoretical perspective, we study how powerful graph neural networks (GNNs) can be for learning approximation algorithms for combinatorial problems. To this end, we first establish a new class of GNNs that can solve strictly a wider variety of problems than existing GNNs. Then, we bridge the gap between GNN theory and the theory of distributed local algorithms to theoretically demonstrate that the most powerful GNN can learn approximation algorithms for the minimum dominating set problem and the minimum vertex cover problem with some approximation ratios and that no GNN can perform better than with these ratios. This paper is the first to elucidate approximation ratios of GNNs for combinatorial problems. Furthermore, we prove that adding coloring or weak-coloring to each node feature improves these approximation ratios. This indicates that preprocessing and feature engineering theoretically strengthen model capabilities.</p>
</blockquote>
<ul>
<li><strong>Key notes</strong>: </li>
</ul>
<hr />
<p>(22) <code>Apr 2019</code> <a href="https://arxiv.org/abs/1904.11088">D-VAE: A Variational Autoencoder for Directed Acyclic Graphs</a> <em>[Muhan Zhang, Shali Jiang, Zhicheng Cui, Roman Garnett, Yixin Chen]</em></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p>Graph structured data are abundant in the real world. Among different graph types, directed acyclic graphs (DAGs) are of particular interest to machine learning researchers, as many machine learning models are realized as computations on DAGs, including neural networks and Bayesian networks. In this paper, we study deep generative models for DAGs, and propose a novel DAG variational autoencoder (D-VAE). To encode DAGs into the latent space, we leverage graph neural networks. We propose an asynchronous message passing scheme that allows encoding the computations on DAGs, rather than using existing simultaneous message passing schemes to encode local graph structures. We demonstrate the effectiveness of our proposed D-VAE through two tasks: neural architecture search and Bayesian network structure learning. Experiments show that our model not only generates novel and valid DAGs, but also produces a smooth latent space that facilitates searching for DAGs with better performance through Bayesian optimization.</p>
</blockquote>
<ul>
<li><strong>Key notes</strong>: <ul>
<li></li>
</ul>
</li>
</ul>
<hr />
<p>(23) <code>May 2019</code> <a href="https://arxiv.org/abs/1905.13732">End to end learning and optimization on graphs</a> <em>[Bryan Wilder, Eric Ewing, Bistra Dilkina, Milind Tambe]</em></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p>Real-world applications often combine learning and optimization problems on graphs. For instance, our objective may be to cluster the graph in order to detect meaningful communities (or solve other common graph optimization problems such as facility location, maxcut, and so on). However, graphs or related attributes are often only partially observed, introducing learning problems such as link prediction which must be solved prior to optimization. We propose an approach to integrate a differentiable proxy for common graph optimization problems into training of machine learning models for tasks such as link prediction. This allows the model to focus specifically on the downstream task that its predictions will be used for. Experimental results show that our end-to-end system obtains better performance on example optimization tasks than can be obtained by combining state of the art link prediction methods with expert-designed graph optimization algorithms.</p>
</blockquote>
<ul>
<li><strong>Key notes</strong>: <ul>
<li></li>
</ul>
</li>
</ul>
<hr />
<p>(24) <code>May 2019</code> <a href="https://arxiv.org/abs/1905.13192">Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels</a> <em>[Simon S. Du, Kangcheng Hou, Barnabás Póczos, Ruslan Salakhutdinov, Ruosong Wang, Keyulu Xu]</em></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p>While graph kernels (GKs) are easy to train and enjoy provable theoretical guarantees, their practical performances are limited by their expressive power, as the kernel function often depends on hand-crafted combinatorial features of graphs. Compared to graph kernels, graph neural networks (GNNs) usually achieve better practical performance, as GNNs use multi-layer architectures and non-linear activation functions to extract high-order information of graphs as features. However, due to the large number of hyper-parameters and the non-convex nature of the training procedure, GNNs are harder to train. Theoretical guarantees of GNNs are also not well-understood. Furthermore, the expressive power of GNNs scales with the number of parameters, and thus it is hard to exploit the full power of GNNs when computing resources are limited. The current paper presents a new class of graph kernels, Graph Neural Tangent Kernels (GNTKs), which correspond to \emph{infinitely wide} multi-layer GNNs trained by gradient descent. GNTKs enjoy the full expressive power of GNNs and inherit advantages of GKs. Theoretically, we show GNTKs provably learn a class of smooth functions on graphs. Empirically, we test GNTKs on graph classification datasets and show they achieve strong performance.</p>
</blockquote>
<ul>
<li><strong>Key notes</strong>: <ul>
<li></li>
</ul>
</li>
</ul>
<hr />
<p>(25) <code>Sep 2018</code> <a href="https://arxiv.org/abs/1809.02589">HyperGCN: A New Method of Training Graph Convolutional Networks on Hypergraphs</a> <em>[Naganand Yadati, Madhav Nimishakavi, Prateek Yadav, Vikram Nitin, Anand Louis, Partha Talukdar]</em></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p>In many real-world network datasets such as co-authorship, co-citation, email communication, etc., relationships are complex and go beyond pairwise. Hypergraphs provide a flexible and natural modeling tool to model such complex relationships. The obvious existence of such complex relationships in many real-world networks naturaly motivates the problem of learning with hypergraphs. A popular learning paradigm is hypergraph-based semi-supervised learning (SSL) where the goal is to assign labels to initially unlabeled vertices in a hypergraph. Motivated by the fact that a graph convolutional network (GCN) has been effective for graph-based SSL, we propose HyperGCN, a novel GCN for SSL on attributed hypergraphs. Additionally, we show how HyperGCN can be used as a learning-based approach for combinatorial optimisation on NP-hard hypergraph problems. We demonstrate HyperGCN&rsquo;s effectiveness through detailed experimentation on real-world hypergraphs.</p>
</blockquote>
<ul>
<li><strong>Key notes</strong>: <ul>
<li></li>
</ul>
</li>
</ul>
<hr />
<p>(26) <code>Jul 2019</code> <a href="https://arxiv.org/abs/1907.03395">Social-BiGAT: Multimodal Trajectory Forecasting using Bicycle-GAN and Graph Attention Networks</a> <em>[Vineet Kosaraju, Amir Sadeghian, Roberto Martín-Martín, Ian Reid, S. Hamid Rezatofighi, Silvio Savarese]</em></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p>Predicting the future trajectories of multiple interacting agents in a scene has become an increasingly important problem for many different applications ranging from control of autonomous vehicles and social robots to security and surveillance. This problem is compounded by the presence of social interactions between humans and their physical interactions with the scene. While the existing literature has explored some of these cues, they mainly ignored the multimodal nature of each human&rsquo;s future trajectory. In this paper, we present Social-BiGAT, a graph-based generative adversarial network that generates realistic, multimodal trajectory predictions by better modelling the social interactions of pedestrians in a scene. Our method is based on a graph attention network (GAT) that learns reliable feature representations that encode the social interactions between humans in the scene, and a recurrent encoder-decoder architecture that is trained adversarially to predict, based on the features, the humans&rsquo; paths. We explicitly account for the multimodal nature of the prediction problem by forming a reversible transformation between each scene and its latent noise vector, as in Bicycle-GAN. We show that our framework achieves state-of-the-art performance comparing it to several baselines on existing trajectory forecasting benchmarks.</p>
</blockquote>
<ul>
<li><strong>Key notes</strong>: <ul>
<li></li>
</ul>
</li>
</ul>
<hr />
<p>(27) <code>May 2019</code> <a href="https://arxiv.org/abs/1905.07645">Scalable Gromov-Wasserstein Learning for Graph Partitioning and Matching</a> <em>[Hongteng Xu, Dixin Luo, Lawrence Carin]</em></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p>We propose a scalable Gromov-Wasserstein learning (S-GWL) method and establish a novel and theoretically-supported paradigm for large-scale graph analysis. The proposed method is based on the fact that Gromov-Wasserstein discrepancy is a pseudometric on graphs. Given two graphs, the optimal transport associated with their Gromov-Wasserstein discrepancy provides the correspondence between their nodes and achieves graph matching. When one of the graphs has isolated but self-connected nodes (i.e., a disconnected graph), the optimal transport indicates the clustering structure of the other graph and achieves graph partitioning. Using this concept, we extend our method to multi-graph partitioning and matching by learning a Gromov-Wasserstein barycenter graph for multiple observed graphs; the barycenter graph plays the role of the disconnected graph, and since it is learned, so is the clustering. Our method combines a recursive K-partition mechanism with a regularized proximal gradient algorithm, whose time complexity is $(K(E+V)logKV)$ for graphs with V nodes and E edges. To our knowledge, our method is the first attempt to make Gromov-Wasserstein discrepancy applicable to large-scale graph analysis and unify graph partitioning and matching into the same framework. It outperforms state-of-the-art graph partitioning and matching methods, achieving a trade-off between accuracy and efficiency.</p>
</blockquote>
<ul>
<li><strong>Key notes</strong>: <ul>
<li></li>
</ul>
</li>
</ul>
<hr />
<p>(28) <code>May 2019</code> <a href="https://arxiv.org/abs/1905.04943">Universal Invariant and Equivariant Graph Neural Networks</a> <em>[Nicolas Keriven, Gabriel Peyré]</em></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p>Graph Neural Networks (GNN) come in many flavors, but should always be either invariant (permutation of the nodes of the input graph does not affect the output) or equivariant (permutation of the input permutes the output). In this paper, we consider a specific class of invariant and equivariant networks, for which we prove new universality theorems. More precisely, we consider networks with a single hidden layer, obtained by summing channels formed by applying an equivariant linear operator, a pointwise non-linearity and either an invariant or equivariant linear operator. Recently, Maron et al. (2019) showed that by allowing higher-order tensorization inside the network, universal invariant GNNs can be obtained. As a first contribution, we propose an alternative proof of this result, which relies on the Stone-Weierstrass theorem for algebra of real-valued functions. Our main contribution is then an extension of this result to the equivariant case, which appears in many practical applications but has been less studied from a theoretical point of view. The proof relies on a new generalized Stone-Weierstrass theorem for algebra of equivariant functions, which is of independent interest. Finally, unlike many previous settings that consider a fixed number of nodes, our results show that a GNN defined by a single set of parameters can approximate uniformly well a function defined on graphs of varying size.</p>
</blockquote>
<ul>
<li><strong>Key notes</strong>: <ul>
<li></li>
</ul>
</li>
</ul>
<hr />
<p>(29) <code>Jun 2019</code> <a href="https://arxiv.org/pdf/1905.11136.pdf">Provably Powerful Graph Networks</a> <em>[Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, Yaron Lipman]</em></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p>Recently, the Weisfeiler-Lehman (WL) graph isomorphism test was used to measure the expressive power of graph neural networks (GNN). It was shown that the popular message passing GNN cannot distinguish between graphs that are indistinguishable by the 1-WL test (Morris et al. 2018; Xu et al. 2019). Unfortunately, many simple instances of graphs are indistinguishable by the 1-WL test.
In search for more expressive graph learning models we build upon the recent k-order invariant and equivariant graph neural networks (Maron et al. 2019a,b) and present two results:
First, we show that such k-order networks can distinguish between non-isomorphic graphs as good as the k-WL tests, which are provably stronger than the 1-WL test for k&gt;2. This makes these models strictly stronger than message passing models. Unfortunately, the higher expressiveness of these models comes with a computational cost of processing high order tensors.
Second, setting our goal at building a provably stronger, simple and scalable model we show that a reduced 2-order network containing just scaled identity operator, augmented with a single quadratic operation (matrix multiplication) has a provable 3-WL expressive power. Differently put, we suggest a simple model that interleaves applications of standard Multilayer-Perceptron (MLP) applied to the feature dimension and matrix multiplication. We validate this model by presenting state of the art results on popular graph classification and regression tasks. To the best of our knowledge, this is the first practical invariant/equivariant model with guaranteed 3-WL expressiveness, strictly stronger than message passing models.</p>
</blockquote>
<ul>
<li><strong>Key notes</strong>: <ul>
<li></li>
</ul>
</li>
</ul>
<hr />
<p>(30) <code>Jun 2017</code> <a href="https://arxiv.org/abs/1706.07845">HARP: Hierarchical Representation Learning for Networks</a> <em>[Haochen Chen, Bryan Perozzi, Yifan Hu, Steven Skiena]</em></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p>We present HARP, a novel method for learning low dimensional embeddings of a graph&rsquo;s nodes which preserves higher-order structural features. Our proposed method achieves this by compressing the input graph prior to embedding it, effectively avoiding troublesome embedding configurations (i.e. local minima) which can pose problems to non-convex optimization. HARP works by finding a smaller graph which approximates the global structure of its input. This simplified graph is used to learn a set of initial representations, which serve as good initializations for learning representations in the original, detailed graph. We inductively extend this idea, by decomposing a graph in a series of levels, and then embed the hierarchy of graphs from the coarsest one to the original graph. HARP is a general meta-strategy to improve all of the state-of-the-art neural algorithms for embedding graphs, including DeepWalk, LINE, and Node2vec. Indeed, we demonstrate that applying HARP&rsquo;s hierarchical paradigm yields improved implementations for all three of these methods, as evaluated on both classification tasks on real-world graphs such as DBLP, BlogCatalog, CiteSeer, and Arxiv, where we achieve a performance gain over the original implementations by up to 14% Macro F1.</p>
</blockquote>
<ul>
<li><strong>Key notes</strong>: <ul>
<li><u><strong>Main contributions</strong></u>: </li>
<li><u><strong>Other Notes</strong></u>:</li>
<li><u><strong>Use cases</strong></u>:</li>
<li><u><strong>Further directions</strong></u>:</li>
</ul>
</li>
</ul>
<hr />
<p>(31) <code>Feb 2017</code> <a href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14589">Community Preserving Network Embedding</a> <em>[Xiao Wang, Peng Cui, Jing Wang, Jian Pei, Wenwu Zhu, Shiqiang Yang]</em></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p>Network embedding, aiming to learn the low-dimensional representations of nodes in networks, is of paramount importance in many real applications. One basic requirement of network embedding is to preserve the structure and inherent properties of the networks. While previous network embedding methods primarily preserve the microscopic structure, such as the first- and second-order proximities of nodes, the mesoscopic community structure, which is one of the most prominent feature of networks, is largely ignored. In this paper, we propose a novel Modularized Nonnegative Matrix Factorization (M-NMF) model to incorporate the community structure into network embedding. We exploit the consensus relationship between the representations of nodes and community structure, and then jointly optimize NMF based representation learning model and modularity based community detection model in a unified framework, which enables the learned representations of nodes to preserve both of the microscopic and community structures. We also provide efficient updating rules to infer the parameters of our model, together with the correctness and convergence guarantees. Extensive experimental results on a variety of real-world networks show the superior performance of the proposed method over the state-of-the-arts.</p>
</blockquote>
<ul>
<li><strong>Key notes</strong>: <ul>
<li><u><strong>Main contributions</strong></u>: </li>
<li><u><strong>Other Notes</strong></u>:</li>
<li><u><strong>Use cases</strong></u>:</li>
<li><u><strong>Further directions</strong></u>:</li>
</ul>
</li>
</ul>
<hr />
<p>(32) <code>Feb 2019</code> <a href="https://arxiv.org/abs/1902.06188">Collaborative Similarity Embedding for Recommender Systems</a> <em>[Chih-Ming Chen, Chuan-Ju Wang, Ming-Feng Tsai, Yi-Hsuan Yang]</em></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p>We present collaborative similarity embedding (CSE), a unified framework that exploits comprehensive collaborative relations available in a user-item bipartite graph for representation learning and recommendation. In the proposed framework, we differentiate two types of proximity relations: direct proximity and k-th order neighborhood proximity. While learning from the former exploits direct user-item associations observable from the graph, learning from the latter makes use of implicit associations such as user-user similarities and item-item similarities, which can provide valuable information especially when the graph is sparse. Moreover, for improving scalability and flexibility, we propose a sampling technique that is specifically designed to capture the two types of proximity relations. Extensive experiments on eight benchmark datasets show that CSE yields significantly better performance than state-of-the-art recommendation methods.</p>
</blockquote>
<ul>
<li><strong>Key notes</strong>: <ul>
<li><u><strong>Main contributions</strong></u>: </li>
<li><u><strong>Other Notes</strong></u>:</li>
<li><u><strong>Use cases</strong></u>:</li>
<li><u><strong>Further directions</strong></u>:</li>
</ul>
</li>
</ul>
<hr />
<p>(33) <code>Apr 2019</code> <a href="https://arxiv.org/abs/1904.12787">Graph Matching Networks for Learning the Similarity of Graph Structured Objects</a> <em>[Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, Pushmeet Kohli]</em></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p>This paper addresses the challenging problem of <strong>retrieval</strong> and <strong>matching</strong> of <em>graph structured objects</em>, and makes two key contributions. </p>
<p>First, we demonstrate how Graph Neural Networks (GNN), which have emerged as an effective model for various supervised prediction problems defined on structured data, can <strong>be trained to produce embedding of graphs in vector spaces that enables efficient similarity reasoning</strong>. </p>
<p>Second, we propose a <strong>novel Graph Matching Network model</strong> that, given a pair of graphs as input, <strong>computes a similarity score between them by jointly reasoning on the pair through a new cross-graph attention-based matching mechanism</strong>. </p>
<p>We demonstrate the effectiveness of our models on different domains including the challenging problem of <strong>control-flow-graph based function similarity search</strong> that plays an important role in the detection of vulnerabilities in software systems. The experimental analysis demonstrates that our models are not only able to exploit structure in the context of similarity learning but they can also outperform domain-specific baseline systems that have been carefully hand-engineered for these problems.</p>
</blockquote>
<ul>
<li>
<p><strong>Key notes</strong>: </p>
<ul>
<li>
<p><u><strong>Main contributions</strong></u>: </p>
<ul>
<li>
<p><strong>[1]</strong> train <strong>GNN</strong> to produce <u>embedding of graphs</u> in vector spaces (<em>graph independently to vector</em>) -&gt; further similarity computation happens in vector space</p>
<ul>
<li><strong>(i) encoder</strong>: node and edge features &ndash;MLPs&ndash;&gt; initial vecotrs</li>
<li><strong>(ii) propagation layers</strong>: set of node representations ----&gt; new representations<ul>
<li><em>without propagation</em> -&gt; <em>Deep set</em> or <em>PointNet</em> (<u>ignore the </u>)</li>
</ul>
</li>
<li><strong>(iii) aggregator</strong>: after T rounds of propagation, aggregate all the node representation to a <strong>graph level representation</strong> ----&gt; <ul>
<li>
<blockquote>
<p>transorms node representations and then uses weighted sum with gating vectors to aggregate across nodes</p>
</blockquote>
</li>
</ul>
</li>
<li>for similarity use: <code>Euclidean</code>, <code>cosine</code>, <code>Hamming</code></li>
</ul>
</li>
<li>
<p><strong>[2]</strong> propose <strong>Graph Matching Network</strong> (GMN) -&gt; for similarity learning</p>
<ul>
<li>compute a similarity score through a <strong>cross-graph attention mechanism</strong> -&gt; <code>associate nodes across graphs and identify differences</code>, <code>use atten-based module</code></li>
<li>more powerful than embedding model</li>
<li>compared to <em>[graph kernel approches]</em>, the authors&rsquo; method based <strong>similarity learning framework</strong> learns the simlarity <strong>end2end</strong></li>
</ul>
</li>
<li>
<p><strong>[3]</strong> evalute the model on three tasks: <code>syntehtic graph edit-distance learning task</code> (capture structural similarity only); real world tasks: <code>binary function similarity search</code> and <code>mesh retrieval</code></p>
</li>
<li>
<p><strong>[Problem 1]</strong>: <u>each cross-graph matching step requires computation of the full attention matrices</u> ----&gt; <strong>expensive for large graph</strong></p>
</li>
<li><strong>[Problem 2]</strong>: the matching models operates on pairs, cannot directly be used for indexing and searching through large databases <ul>
<li><u><strong>Other Notes</strong></u>:</li>
</ul>
</li>
<li>graphs -&gt; encoding <strong>relational structures</strong></li>
<li><strong>Graph kernel</strong>: kernels on graphs designed to capture the <strong>graph similarity</strong>, can be used in kernel method for <code>graph classifcation</code><ul>
<li>kernels based on <strong>limited-sized</strong> sub-strucutures</li>
<li>kernels based on <strong>sub-tree</strong> structures</li>
<li>Can ge formulated: <ul>
<li>
<blockquote>
<p>[i] computig the features vector for each graph (the kernel embedding)</p>
</blockquote>
</li>
<li>
<blockquote>
<p>[ii] take inner product between vectors to compute kernel values</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
<li><u>distance metric learning</u>:<ul>
<li>[early work] assumes data already lies in a vector space -&gt; only a linear metric metrix is learned</li>
<li>[more recently] been combined in <code>face verification</code> (CNN to map similar images to similar vectors)</li>
<li>[this work] modeling cross-graph matchings</li>
</ul>
</li>
<li><u>Graph edit distance</u>: the minimum number of edit operations needed to transform G1 to G2 (<em>add/removes/substitute</em>)</li>
</ul>
</li>
<li>
<p><u><strong>Use cases</strong></u>:</p>
<ul>
<li><u>control-flow-graph based function similarity search</u> -&gt; <strong>detection of vulnerabilities in software systems</strong><ul>
<li><u>In the past</u>: use calssical graph theoretical matching algorithm</li>
</ul>
</li>
</ul>
</li>
<li><u><strong>Further directions</strong></u>:<ul>
<li><strong>[1]</strong> improve the efficiency of the matching modesl</li>
<li><strong>[2]</strong> study different matching architectures</li>
<li><strong>[3]</strong> adapt GNN capacity to application domains</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr />
<p>(34) <code>Jul 2016</code> <a href="https://arxiv.org/abs/1607.00653">node2vec: Scalable Feature Learning for Networks</a> <em>[Aditya Grover, Jure Leskovec]</em></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote>
<p>Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of <strong>representation learning</strong> has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node&rsquo;s network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the <strong>added flexibility in exploring neighborhoods</strong> is the key to learning richer representations. We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on <strong>multi-label classification</strong> and <strong>link prediction</strong> in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks.</p>
</blockquote>
<ul>
<li><strong>Key notes</strong>: <ul>
<li><u><strong>Main contributions</strong></u>: <ul>
<li><strong>[i]</strong> propose <strong>node2vec</strong> -&gt; <code>efficient</code>, <code>scalable</code> algorithm for feature learning optimizes a <code>novel</code>, <code>network-aware</code>, <code>neighborhood preserving</code> objective using <strong>SGD</strong><h2 id="-use-a-2nd-order-random-walk-approach-to-generate-sample-network-neighborhoods-for-nodes">- <em>use a 2nd order <u>random walk</u> approach to generate sample network neighborhoods for nodes</em><a class="headerlink" href="#-use-a-2nd-order-random-walk-approach-to-generate-sample-network-neighborhoods-for-nodes" title="Permanent link"></a></h2>
</li>
<li><strong>[ii]</strong> show node2vec in accordance with <strong>principles in network science</strong> -&gt; [providing flexibility in discovering representations conforming to different equivalences]</li>
<li><strong>[iii]</strong> extend node2vec (and other feature learning methods) based on <strong>neighborhood preserving objectives</strong>, from node -&gt; pair of nodes (edge prediction tasks)</li>
<li><strong>[iv]</strong> evaluate node2vec on <code>multi-lable classification</code> and <code>link prediction</code></li>
</ul>
</li>
<li><u><strong>Other Notes</strong></u>:<ul>
<li>challenge in <strong>feature representations</strong>: <u>defining an objective function</u> [trade-off in balancing computational efficency and predictive accurcy]</li>
<li>essential points for a flexible algorithm to learn node representation<ul>
<li><strong>[1]</strong> ability to learn representatins that embed nodes from the same network community closely together</li>
<li><strong>[2]</strong> learn representations where nodes that share similar roles have similar embeddingss</li>
</ul>
</li>
<li>the conventional <u>dimensionality reduction techniques</u> have drawbacks in: <ol>
<li>computational &amp; statistical performance</li>
<li>optimize for objectives that are <strong>not robust</strong> to the diverse pattern observed in networks <em>(assumptions -&gt; relationship between underlying network structure and the prediction task)</em></li>
</ol>
</li>
<li>traditional search strategies: <code>Breadth-first sampling</code>, <code>Depth-first Sampling</code></li>
<li>prediction tasks with two similarities: <ul>
<li>
<blockquote>
<p><strong>homophily hypothesis</strong>: nodes that are highly interconnected and belong to simliar network clusters or communities should be embedded closely together</p>
</blockquote>
</li>
<li>
<blockquote>
<p><strong>structural equivalence hypothesis</strong>: nodes have similar structural roles in networks should be embedded closely together</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
<li><u><strong>Use cases</strong></u>:<ol>
<li>predicting interests of users in a social network</li>
<li>predicting functional labels of protein in protein-protein interatction network</li>
<li>link prediction<ul>
<li>discover novel interactions between genes in genomics</li>
<li>identify real-world friends in social network</li>
</ul>
</li>
</ol>
</li>
<li><u><strong>Further directions</strong></u>:</li>
</ul>
</li>
</ul>
<hr />
<p>(28) <code>201</code> <a href=""></a> <em>[]</em></p>
<ul>
<li><strong>Abstract</strong>: </li>
</ul>
<blockquote></blockquote>
<ul>
<li><strong>Key notes</strong>: <ul>
<li><u><strong>Main contributions</strong></u>: </li>
<li><u><strong>Other Notes</strong></u>:</li>
<li><u><strong>Use cases</strong></u>:</li>
<li><u><strong>Further directions</strong></u>:</li>
</ul>
</li>
</ul>
<hr />
<h3 id="snarp-paper-to-read">SNA/RP paper to read<a class="headerlink" href="#snarp-paper-to-read" title="Permanent link"></a></h3>
<ul>
<li>(dynamic/embedding)<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16572">Dynamic Network Embedding by Modeling Triadic Closure Process</a></li>
<li>(dynamic/embedding)<a href="https://arxiv.org/pdf/1706.01860.pdf">Attributed Network Embedding for Learning in a Dynamic Environment</a></li>
<li>(Influence Max/Time)<a href="https://arxiv.org/abs/1905.06618">On the Fairness of Time-Critical Influence Maximization in Social Networks</a></li>
<li>(Embedding/link prediction)<a href="https://ieeexplore.ieee.org/abstract/document/8326519">Attributed Social Network Embedding</a></li>
<li>(Dynamic network) <a href="https://arxiv.org/abs/1906.03586">Dynamic Network Embedding via Incremental Skip-gram with Negative Sampling</a></li>
<li>(Dynamic network) <a href="https://www.ijcai.org/proceedings/2018/0288.pdf">Dynamic Network Embedding: An Extended Approach for Skip-gram based Network Embedding</a></li>
<li>(x)(Dynamic/Influen max) <a href="https://link.springer.com/chapter/10.1007/978-3-030-20482-2_6">Influential Nodes Detection in Dynamic Social Networks</a></li>
<li>(x)(Dyanmic/Influen max) <a href="https://www.sciencedirect.com/science/article/abs/pii/S0096300319301602">TIFIM: A Two-stage Iterative Framework for Influence Maximization in Social Networks</a></li>
<li>(x)(Dynamic/Link prediction) <a href="https://link.springer.com/chapter/10.1007/978-3-319-94268-1_70">A Supervised Learning Approach to Link Prediction in Dynamic Networks</a></li>
<li>(x)(Time Series/Link prediction/Heterogeneous)<a href="https://ideas.repec.org/a/wsi/ijitdm/v18y2019i01ns0219622018500530.html">Multivariate Time Series Link Prediction for Evolving Heterogeneous Network</a></li>
<li>(Dynamic/Link Prediction)<a href="https://arxiv.org/pdf/1610.04351.pdf">Semi–supervised Graph Embedding Approach to Dynamic Link Prediction</a></li>
</ul>
<h3 id="useful-for-network-analysis-research">Useful for network analysis research:<a class="headerlink" href="#useful-for-network-analysis-research" title="Permanent link"></a></h3>
<ul>
<li><a href="https://www.frontiersin.org/articles/10.3389/fdata.2019.00002/full">Deep Representation Learning for Social Network Analysis</a></li>
<li><a href="https://snap.stanford.edu/data/">Stanford Large Network Dataset Collection</a></li>
<li><a href="https://www.youtube.com/watch?v=px7ff2_Jeqw&amp;list=WL&amp;index=29&amp;t=0s">[video] Social Network Analysis - From Graph Theory to Applications - Dima Goldenberg - PyCon Israel 2019</a></li>
<li><a href="https://cs.stanford.edu/people/jure/pubs/gin-iclr19.pdf">HOW POWERFUL ARE GRAPH NEURAL NETWORKS?</a></li>
<li><a href="https://earli.org/sites/default/files/2017-03/ASC2018-web.pdf">Social Network Analysis in the field of Learning and Instruction: methodological issues and advances</a></li>
<li><a href="https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/491572/socnet_howto.pdf">Social Network Analysis: ‘How to guide’</a></li>
<li><a href="https://www.sciencedirect.com/science/article/pii/S0167739X17307938">User behavior prediction in social networks using weighted extreme learning machine with distribution optimization</a></li>
<li><a href="https://link.springer.com/article/10.1007/s41109-019-0134-3">Analyzing and inferring human real-life behavior through online social networks with social influence deep learning</a></li>
</ul>
<p><strong><u>Mihai Cucuringu (ox)</u></strong> </p>
<ul>
<li><a href="http://www.stats.ox.ac.uk/~cucuring/signedClustering.pdf">SPONGE: A generalized eigenproblem for clustering signed networks</a></li>
<li><a href="http://www.stats.ox.ac.uk/~cucuring/anomaly_detection_networks.pdf">Anomaly Detection in Networks with Application to Financial Transaction Networks</a></li>
</ul>
<p><strong><u>François Caron (ox)</u></strong> </p>
<ul>
<li>X. Miscouridou, F. Caron, Y. W. Teh. <a href="https://papers.nips.cc/paper/7502-modelling-sparsity-heterogeneity-reciprocity-and-community-structure-in-temporal-interaction-data.pdf">Modelling sparsity, heterogeneity, reciprocity and community structure in temporal interaction data</a>. Neural Information Processing Systems (NeurIPS&lsquo;2018), Montreal, Canada, 2018. </li>
</ul>
<p><strong><u>Rik Sarkar (Edinburgh)</u>:</strong></p>
<ul>
<li>Benedek Rozemberczki, Ryan Davies, Rik Sarkar, Charles Sutton. <a href="http://homepages.inf.ed.ac.uk/rsarkar/papers/gemsec.pdf">GEMSEC: Graph Embedding with Self Clustering</a>, IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM) 2019.</li>
<li>Panagiota Katsikouli, Maria Sinziana Astefanoaei, Rik Sarkar. <a href="http://homepages.inf.ed.ac.uk/rsarkar/papers/popular_paths_dcoss.pdf">Distributed Mining of Popular Paths in Road Networks</a>, IEEE International Conference on Distributed Computing in Sensor Systems 2018 (DCOSS &lsquo;18).</li>
</ul>
<p><strong><u>Walid Magdy (Edinburgh)</u></strong></p>
<ul>
<li><a href="https://arxiv.org/abs/1907.12700">A Practical Guide for the Effective Evaluation of Twitter User Geolocation</a></li>
<li><a href="https://aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/view/17833">Self-Representation on Twitter Using Emoji Skin Color Modifiers</a></li>
</ul>
<p><strong><u>Kobi Gal (Edinburgh)</u></strong></p>
<ul>
<li>Avi Segal, Kobi  Gal, Guy Shani and Bracha Shapira. <a href="https://arxiv.org/abs/1907.12047">A difficulty Ranking Approach to Personalization in E-learning</a>. International Journal of Human Computer Studies 130: 261-272, 2019. Supercedes the EDM-14 paper below. </li>
</ul>
<p><strong><u>Timothy M. Hospedales (Edinburgh)</u></strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/1901.11448.pdf">Feature-Critic Networks for Heterogeneous Domain Generalisation</a></li>
</ul>
<p><strong><u>Emine Yilmaz (UCL)</u></strong></p>
<ul>
<li><a href="https://dl.acm.org/citation.cfm?id=3020188">User Behaviour and Task Characteristics: A Field Study of Daily Information Behaviour</a></li>
<li><a href="http://delivery.acm.org/10.1145/3190000/3186919/p41-zhang.pdf?ip=180.154.9.45&amp;id=3186919&amp;acc=OPEN&amp;key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E6D218144511F3437&amp;__acm__=1571069502_abde5d91c360787561f71b95266fbe84">Ranking-based Method for News Stance Detection</a></li>
</ul>
<p><strong><u>Iván Palomares Carrascosa (Bristol)</u></strong></p>
<ul>
<li><a href="https://www.sciencedirect.com/science/article/abs/pii/S0377221718310191?via%3Dihub">Large-scale group decision making model based on social network analysis: Trust relationship-based conflict detection and elimination</a></li>
<li>H. Zhang, I. Palomares, Y.C. Dong, W. Wang. <a href="https://doi.org/10.1016/j.knosys.2018.06.008">Managing non-cooperative behaviors in consensus-based multiple attribute group decision making: An approach based on social network analysis</a>. Knowledge-based Systems, 162, pp. 29-45, 2018.</li>
<li>Z. Zhang, X. Kou, I. Palomares, W. Yu, J. Gao. <a href="https://doi.org/10.1016/j.asoc.2019.105730">Stable two-sided matching decision making with incomplete fuzzy preference relations: A disappointment theory based approach</a>. Applied Soft Computing. In press: </li>
</ul>
<h3 id="to-do-for-graph-network">To do for graph &amp; network<a class="headerlink" href="#to-do-for-graph-network" title="Permanent link"></a></h3>
<p>Li, Y., Vinyals, O., Dyer, C., Pascanu, R., and Battaglia,
P. Learning deep generative models of graphs. arXiv
preprint arXiv:1803.03324, 2018.</p>
<p>Wang, T., Liao, R., Ba, J., and Fidler, S. Nervenet: Learning
structured policy with graph neural networks. In ICLR,
2018a.</p>
<p>Al-Rfou, R., Zelle, D., and Perozzi, B. Ddgk: Learning graph representations for deep divergence graph kernels.
arXiv preprint arXiv:1904.09671, 2019.</p>
<ul class="task-list">
<li class="task-list-item"><input type="checkbox" disabled/> W. Hamilton, R. Ying, and J. Leskovec. Inductive representation learning on large graphs. In NIPS, 2017. (<em>negative sampling</em>)</li>
<li class="task-list-item"><input type="checkbox" disabled/> S. Abu-El-Haija, B. Perozzi, and R. Al-Rfou. Learning edge representations via low-rank asymmetric projections. In ACM International Conference on Information and Knowledge Management (CIKM), 2017. (<em>graph likelihood</em>)</li>
<li class="task-list-item"><input type="checkbox" disabled/> N.Shervashidze,P.Schweitzer,E.J.v.Leeuwen,K.Mehlhorn,andK.M.Borgwardt.Weisfeiler- lehman graph kernels. Journal of Machine Learning Research, 12:2539–2561, 2011. (<em>Graph Kernel</em>)</li>
<li class="task-list-item"><input type="checkbox" disabled/> B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In KDD, 2014. (Deep Walk)</li>
<li class="task-list-item"><input type="checkbox" disabled/> Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. arXiv preprint arXiv:1611.08402, 2016. (<em>MoNet</em>)</li>
<li class="task-list-item"><input type="checkbox" disabled/> H. Chen, B. Perozzi, R. Al-Rfou, and S. Skiena. A tutorial on network embeddings</li>
</ul>
<p><strong>Knowledge Graph Related</strong>:</p>
<ul class="task-list">
<li class="task-list-item"><input type="checkbox" disabled/> <a href="https://www.aclweb.org/anthology/P19-1466">Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs</a></li>
</ul>
<p><strong>Other paper</strong>:</p>
<ul class="task-list">
<li class="task-list-item">
<p><input type="checkbox" disabled/> T. N. Kipf and M. Welling. Variational graph auto-encoders. In NIPS Workshop on Bayesian
Deep Learning, 2016.</p>
</li>
<li class="task-list-item">
<p><input type="checkbox" disabled/> Sami Abu-El-Haija. 2017. Proportionate gradient updates with PercentDelta. In arXiv. [graph likelihood]</p>
</li>
</ul></article></body></html>